{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ak3SqhmRDYKW"
   },
   "source": [
    "## CSCE 676 :: Data Mining and Analysis :: Texas A&M University :: Spring 2026\n",
    "\n",
    "\n",
    "# Weekly Homework 3: Graphs!\n",
    "\n",
    "\n",
    "***Goals of this homework:***\n",
    "Perform an analysis of a graph of your choice.\n",
    "\n",
    "\n",
    "***Submission instructions:***\n",
    "\n",
    "You should post your notebook to Canvas (look for the assignment there). Please name your submission **your-uin_hw3.ipynb**, so for example, my submission would be something like **555001234_hw3.ipynb**. Your notebook should be fully executed when you submit ... so run all the cells for us so we can see the output, then submit that.\n",
    "\n",
    "***Grading philosophy:***\n",
    "\n",
    "We are grading reasoning, judgment, and clarity, not just correctness. Show us that you understand the data, the constraints, and the limits of your conclusions.\n",
    "\n",
    "***For each question, you need to respond with 2 cells:***\n",
    "1. **[A Code Cell] Your Code:**\n",
    "  - If code is not applicable for the question, you can skip this cell.\n",
    "  - For tests: tests can be simple assertions or checks (e.g., using `assert` or `print` or small functions or visual inspection); formal testing frameworks are not required.\n",
    "2. **[A Markdown Cell] Your Answer:** Write up your answers and explain them in complete sentences. Include any videos in this section as well; for videos, upload them to your TAMU Google Drive, and ensure they are set to be visible by the instruction team (set to: **anyone with a TAMU email can view**), then share the link to the video in the cell.\n",
    "\n",
    "***At the end of each Section (A/B/C/...) include a cell for your resources:***\n",
    "\n",
    "**[A Markdown Cell] Your Resources:** You need to cite 3 types of resources and note how they helped you: (1) Collaborators, (2) Web Sources (e.g. StackOverflow), and (3) AI Tools (you must also describe how you prompted, but we do not require any links to any specific chats). Specifically, use the following format as a template:\n",
    "```\n",
    "On my honor, I declare the following resources:\n",
    "1. Collaborators:\n",
    "- Reveille A.: Helped me understand that a df in pandas is a data structure kinda like a CSV.\n",
    "- Sully A.: Helped me fix a bug with the vector addition of 2 columns.\n",
    "- ...\n",
    "\n",
    "2. Web Sources:\n",
    "- https://stackoverflow.com/questions/46562479/python-pandas-data-frame-creation: how to create a pd df\n",
    "- ...\n",
    "\n",
    "3. AI Tools:\n",
    "- ChatGPT: I gave it the homework .ipynb file and the ufo.csv, and told it to generate the code for the first question, but it did it with csv.reader(), so I re-prompted it to use pandas and that one was correct\n",
    "- ...\n",
    "```\n",
    "***Why do we require this cell?*** This cell is important...\n",
    "\n",
    "1. For academic integrity, you must give credit where credit is due.\n",
    "\n",
    "2. We want you to pay attention to how you can successfully get help to move through problems! Is there someone you work with or an AI tool that helps you learn the material better? That's great! The point of engineering is to use your tools to solve hard problems, and part of graduate school is learning about how *you* learn and solve problems best.\n",
    "\n",
    "***A reminder: you get out of it what you put into it.***\n",
    "Do your best on these homeworks, show us your creativity, and ask for help when you need it -- good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adEgfyRL-Bkk"
   },
   "source": [
    "# A [72pts]. Step-by-Step Data Mining & Experimental Analysis on A Graph of Your Choice\n",
    "\n",
    "**Rubric**\n",
    "\n",
    "[18 pts] Strong/Professional: Correct and complete implementation of the task; Reasonable assumptions, stated or implied, and justified; Thoughtful handling of real-world data issues (missingness, noise, scale, duplicates, edge cases); Clear, concise explanations of what was done and why; Code is clean, readable, and well-structured, uses appropriate pandas, and would plausibly pass a professional code review; Tests meaningfully validate non-trivial behavior (not just \"the code runs so it must be right\").\n",
    "\n",
    "[9 pts] Partial/Developing: Core task mostly completed but with gaps, weak assumptions, or minor mistakes; Reasoning is shallow or mostly descriptive; Code works but is messy, repetitive, or fragile; Tests are superficial, incomplete, or poorly motivated.\n",
    "\n",
    "[0 pts] Minimal/Incorrect: Task is largely incorrect, missing, or misunderstands the goal; Little to no reasoning or justification; Code does not run or ignores constraints; No meaningful tests.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILBA0EepHf5g"
   },
   "source": [
    "## Overview\n",
    "In this homework, you will **choose one dataset you like** from the [SNAP datasets](https://snap.stanford.edu/data/index.html) collection from the **Social networks** section. You must choose a  **directed graph only**. In this section, you will perform a step‑by‑step data mining & experimental analysis. *Much of this section is self-directed, meaning you will need to make critical decisions about what tools you use and what you explore.*\n",
    "\n",
    "Ideally, you should eventually turn in a coherent story: **What you tried → Why → What you found → So what? → Wait...Anything more?**. It's completely OKAY if you only get minor discoveries. But you should always document the whole learning and reasoning process. Grading will be based on the logic and coherence of your submitted notebook.\n",
    "\n",
    "As a guide, for each step of the homework, you should briefly document:\n",
    "- **Method choice & rationale.** Why this method? What do you expect?\n",
    "- **Parameters.** E.g., `alpha=0.85` for PageRank; seed selection for PPR; thresholds for community extraction.\n",
    "- **Results.** Tables/plots + **1–3 sentences** of interpretation.\n",
    "- **Reflection.** Did results match your expectations? If not, why might that be?\n",
    "\n",
    "Ideally, strong submissions should read like a *short research memo* rather than a raw dump of numbers.\n",
    "\n",
    "## Environment Setup\n",
    "You may use **Python 3.9+**. Other possible tools are:\n",
    "- **Graph tools**: You are welcome to use existing tools that are optimised for graph learning. Among them, `networkx`is recommended. `graph-tool` and `igraph` are also good when dealing with larger graphs.\n",
    "- **Data processing packages**: eg. `numpy`, `pandas`\n",
    "- **Plotting**: `matplotlib`, `plotly`, `seaborn` etc.\n",
    "- **other related tools** `scipy`, `scikit-learn`,  `pytorch` etc.\n",
    "- You may choose other tools; just be sure to let us know.\n",
    "\n",
    "> If your chosen dataset is very large, consider using `graph-tool` and `igraph`, or sampling/induced subgraphs to stay within reasonable time/memory limits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "q4e2DfNwHtgW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2.1\n"
     ]
    }
   ],
   "source": [
    "# Install libraries as needed (uncomment if running on a clean environment)\n",
    "# %pip install networkx pandas numpy matplotlib scipy scikit-learn\n",
    "# %pip install python-louvain igraph\n",
    "\n",
    "import os, io, gzip, zipfile, tarfile, sys, math, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "import shutil\n",
    "import igraph as ig\n",
    "import csv\n",
    "# For reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(nx.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYitLmEqHgAi"
   },
   "source": [
    "# 1. Choose a Dataset\n",
    "Pick **one** dataset from SNAP's **Directed networks** (e.g.`ego-Twitter`, `wiki-Vote` samples, etc.). Paste the **download URL** and a brief description of why you chose it.\n",
    "\n",
    "- **Dataset name:** _e.g., soc-Slashdot0811_\n",
    "- **URL:** _direct link to .txt/.gz_\n",
    "- **Why this dataset?** _1–3 sentences on interest & expected properties_\n",
    "\n",
    "> ⚠️ Make sure it's **directed**.\n",
    "\n",
    "Add this cite to your citation cell:\n",
    "> Jure Leskovec and Andrej Krevl. SNAP Datasets: Stanford Large Network Dataset Collection. http://snap.stanford.edu/data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_stZ0KDDHyA7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://snap.stanford.edu/data/soc-Epinions1.txt.gz ...\n",
      "Saved to ../data/raw_graph.txt.gz (1.55 MB)\n",
      "Downloading from http://fake.com ...\n",
      "Saved to /tmp/tmpy68jvuww (0.00 MB)\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "#NO ASSUMPTIONS MADE!\n",
    "# === You may using the follow method to download datasets if you like ===\n",
    "DATA_URL = \"https://snap.stanford.edu/data/soc-Epinions1.txt.gz\"  # Example; replace with your chosen dataset\n",
    "LOCAL_PATH = \"../data/raw_graph.txt.gz\"\n",
    "\n",
    "def download_dataset(url: str, to_path: str):\n",
    "    import urllib.request\n",
    "    print(f\"Downloading from {url} ...\")\n",
    "    urllib.request.urlretrieve(url, to_path)\n",
    "    size = os.path.getsize(to_path) / (1024*1024)\n",
    "    print(f\"Saved to {to_path} ({size:.2f} MB)\")\n",
    "\n",
    "# Uncomment to download when ready\n",
    "download_dataset(DATA_URL, LOCAL_PATH)\n",
    "#TESTS\n",
    "import os\n",
    "import re\n",
    "import gzip\n",
    "import pytest\n",
    "\n",
    "def _write_dummy_gz(path: str, size_bytes: int = 1024) -> None:\n",
    "    \"\"\"Create a gz file of approximately size_bytes (uncompressed isn't important here).\"\"\"\n",
    "    # We'll just write enough bytes into a gzip file to ensure os.path.getsize works.\n",
    "    payload = b\"a\" * size_bytes\n",
    "    with gzip.open(path, \"wb\") as f:\n",
    "        f.write(payload)\n",
    "\n",
    "\n",
    "import tempfile\n",
    "\n",
    "def test_basic_download_mock():\n",
    "    with tempfile.NamedTemporaryFile() as f:\n",
    "        path = f.name\n",
    "\n",
    "        def fake_urlretrieve(url, to_path):\n",
    "            with open(to_path, \"wb\") as out:\n",
    "                out.write(b\"12345\")\n",
    "\n",
    "        urllib.request.urlretrieve = fake_urlretrieve\n",
    "\n",
    "        download_dataset(\"http://fake.com\", path)\n",
    "\n",
    "        assert os.path.exists(path)\n",
    "        assert os.path.getsize(path) > 0\n",
    "\n",
    "test_basic_download_mock()\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Dataset name:** Epinions Social Network\n",
    "- **URL:** https://snap.stanford.edu/data/soc-Epinions1.txt.gz\n",
    "- **Why this dataset?** I chose this dataset because it is reasonably sized (it can fit in main memory) and it is a directed graph. It will also be pretty easy to interpret the results of pagerank since each edge (a, b) means a trusts b. Thus, page rank in this graph simply implies that a given user is trustworthy: someone who is trusted by other people, who are themselves trusted, will have a high page rank.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYQCrQNyHyRS"
   },
   "source": [
    "# 2. Load & Parse the Directed Graph\n",
    "First choose a graph tool and write a few lines of why you choose it and how you are going to use it. If you are not using any tools, please also document that.\n",
    "\n",
    "> SNAP directed edge lists are usually in the form `src\\t dst` per line, with comments starting with `#`.\n",
    "\n",
    "Implement a robust loader that:\n",
    "- Skips comment lines (if there is any)\n",
    "- Builds a **`networkx.DiGraph`** (if you choose networkx)\n",
    "- (Optional) Restricts to the **largest weakly connected component (WCC)** for clarity\n",
    "\n",
    "Write your loading graph code here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "mlaYqHjNH5u5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decompressing ../data/raw_graph.txt.gz ...\n",
      "Saved decompressed file to ../data/raw_graph.txt\n",
      "Loaded the raw epinion graph which has  508837  edges and  75879  vertices\n",
      "✅ test_load_directed_graph_csv_separator passed!\n",
      "✅ test_load_directed_graph_basic passed!\n",
      "Decompressing /tmp/tmpfu48m6z4/test.txt.gz ...\n",
      "Saved decompressed file to /tmp/tmpfu48m6z4/test.txt\n",
      "gunzip_file test passed!\n"
     ]
    }
   ],
   "source": [
    "def gunzip_file(file_path):\n",
    "    \"\"\"\n",
    "    Decompress a .gz file into the same directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        Path to the .gz file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    output_path : str\n",
    "        Path to the decompressed file\n",
    "    \"\"\"\n",
    "\n",
    "    # Make sure the file ends in .gz\n",
    "    if not file_path.endswith(\".gz\"):\n",
    "        raise ValueError(\"Input file must end with .gz\")\n",
    "\n",
    "    # Output path = remove .gz extension\n",
    "    output_path = file_path[:-3]\n",
    "\n",
    "    print(f\"Decompressing {file_path} ...\")\n",
    "\n",
    "    # Open compressed file and write decompressed version\n",
    "    with gzip.open(file_path, \"rb\") as f_in:\n",
    "        with open(output_path, \"wb\") as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "    print(f\"Saved decompressed file to {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "RAW_GRAPH_PATH = gunzip_file(LOCAL_PATH)\n",
    "    \n",
    "\n",
    "\n",
    "def load_directed_graph(file_path, separator: str = \" \") -> igraph.Graph:\n",
    "    \"\"\"Load a directed edge list into a igraph.Graph.\n",
    "    Assumes lines like: u<sep>v, with comment lines starting by `comment`.\n",
    "    If sep is None, split on whitespace.\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Notably, the igraph library has a lot of functionality, however reading a graph from a csv is not apart of that functionality.\n",
    "    igraph.Graph constructor: __init__(n=0, edges=None, directed=False, graph_attrs=None, vertex_attrs=None, edge_attrs=None)\n",
    "    igraph.Edge constructor: \n",
    "    \"\"\"\n",
    "    node2id = {}\n",
    "    edges = []\n",
    "    def get_id(label: str) -> int:\n",
    "        if label not in node2id:\n",
    "            node2id[label] = len(node2id)\n",
    "        return node2id[label]\n",
    "\n",
    "    with open(file_path, \"rt\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith(\"#\"):\n",
    "                continue\n",
    "\n",
    "            if separator is None:\n",
    "                parts = line.split()\n",
    "            else:\n",
    "                parts = line.split(separator)\n",
    "\n",
    "            # Some datasets may have extra columns; only take first two\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "\n",
    "            u, v = parts[0], parts[1]\n",
    "            edges.append((get_id(u), get_id(v)))\n",
    "\n",
    "    graph = igraph.Graph(n=len(node2id), edges=edges, directed=True)\n",
    "    # Store original labels so you can map back later\n",
    "    graph.vs[\"name\"] = [None] * len(node2id)\n",
    "    for label, idx in node2id.items():\n",
    "        graph.vs[idx][\"name\"] = label\n",
    "\n",
    "    return graph\n",
    "\n",
    "raw_epinion_graph = load_directed_graph(RAW_GRAPH_PATH, separator=\"\\t\")\n",
    "print(\"Loaded the raw epinion graph which has \", raw_epinion_graph.ecount(), \" edges and \", raw_epinion_graph.vcount(), \" vertices\")\n",
    "#TESTS\n",
    "def test_gunzip():\n",
    "    import tempfile\n",
    "\n",
    "    # Create dummy gz file\n",
    "    tmp_dir = tempfile.mkdtemp()\n",
    "    gz_file = os.path.join(tmp_dir, \"test.txt.gz\")\n",
    "\n",
    "    with gzip.open(gz_file, \"wb\") as f:\n",
    "        f.write(b\"hello world\")\n",
    "\n",
    "    # Run gunzip\n",
    "    out_file = gunzip_file(gz_file)\n",
    "\n",
    "    # Check contents\n",
    "    with open(out_file, \"rb\") as f:\n",
    "        assert f.read() == b\"hello world\"\n",
    "\n",
    "    print(\"gunzip_file test passed!\")\n",
    "\n",
    "\n",
    "def test_load_directed_graph_basic():\n",
    "    # Create a temporary edge list file\n",
    "    content = \"\"\"# This is a comment\n",
    "    1 2\n",
    "    2 3\n",
    "    \n",
    "    # Another comment\n",
    "    3 1\n",
    "    \"\"\"\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(mode=\"w+\", delete=False) as f:\n",
    "        f.write(content)\n",
    "        path = f.name\n",
    "\n",
    "    # Load graph\n",
    "    G = load_directed_graph(path)\n",
    "\n",
    "    # Assertions\n",
    "    assert G.is_directed(), \"Graph should be directed\"\n",
    "    assert G.vcount() == 3, f\"Expected 3 vertices, got {G.vcount()}\"\n",
    "    assert G.ecount() == 3, f\"Expected 3 edges, got {G.ecount()}\"\n",
    "\n",
    "    # Check vertex names exist\n",
    "    assert set(G.vs[\"name\"]) == {\"1\", \"2\", \"3\"}\n",
    "\n",
    "    print(\"✅ test_load_directed_graph_basic passed!\")\n",
    "\n",
    "def test_load_directed_graph_csv_separator():\n",
    "    content = \"\"\"A,B\n",
    "B,C\n",
    "C,A\n",
    "\"\"\"\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(mode=\"w+\", delete=False) as f:\n",
    "        f.write(content)\n",
    "        path = f.name\n",
    "\n",
    "    # Load graph with comma separator\n",
    "    G = load_directed_graph(path, separator=\",\")\n",
    "\n",
    "    assert G.is_directed()\n",
    "    assert G.vcount() == 3\n",
    "    assert G.ecount() == 3\n",
    "\n",
    "    assert set(G.vs[\"name\"]) == {\"A\", \"B\", \"C\"}\n",
    "\n",
    "    print(\"✅ test_load_directed_graph_csv_separator passed!\")\n",
    "\n",
    "test_load_directed_graph_csv_separator()\n",
    "test_load_directed_graph_basic()\n",
    "test_gunzip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am choosing to use the igraph tool for graphs because it has some very useful built in analysis functions such as betweeness(), closeness(), and even pagerank(). I'm going to use this tool for analysis as well as visualization since the library also provides visualization functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "or5kYkRjH8yf"
   },
   "source": [
    "# 3. First Look: Basic Structural Statistics\n",
    "Compute and report at least:\n",
    "- `|V|` (nodes), `|E|` (edges)\n",
    "- **Average in/out degree**, **degree distributions** (plot)\n",
    "- **#SCCs**, size of **largest SCC** and **largest WCC**\n",
    "- *(Optional): **Density**, **reciprocity***\n",
    "\n",
    "Add a few sentences interpreting what these numbers suggest about your network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "bCK1sVWPH-XC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPINION STATS\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_vertices</th>\n",
       "      <th>num_edges</th>\n",
       "      <th>num_scc</th>\n",
       "      <th>largest_scc</th>\n",
       "      <th>num_wcc</th>\n",
       "      <th>largest_wcc</th>\n",
       "      <th>density</th>\n",
       "      <th>reciprocity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75879</td>\n",
       "      <td>508837</td>\n",
       "      <td>42176</td>\n",
       "      <td>32223</td>\n",
       "      <td>2</td>\n",
       "      <td>75877</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.405226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_vertices  num_edges  num_scc  largest_scc  num_wcc  largest_wcc  \\\n",
       "0         75879     508837    42176        32223        2        75877   \n",
       "\n",
       "    density  reciprocity  \n",
       "0  0.000088     0.405226  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ test_basic_stats passed!\n"
     ]
    }
   ],
   "source": [
    "'''sample codes'''\n",
    "import igraph as ig\n",
    "import pandas as pd\n",
    "\n",
    "def basic_stats(graph: ig.Graph) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute basic directed-graph statistics and return them\n",
    "    as a one-row pandas DataFrame.\n",
    "\n",
    "    Stats included:\n",
    "      - number of vertices, number of edges\n",
    "      - number of SCCs, size of largest SCC\n",
    "      - number of WCCs, size of largest WCC\n",
    "      - density\n",
    "      - reciprocity\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Basic size ---\n",
    "    n = graph.vcount()\n",
    "    m = graph.ecount()\n",
    "\n",
    "    # --- Connected components ---\n",
    "    scc = graph.components(mode=\"STRONG\")\n",
    "    wcc = graph.components(mode=\"WEAK\")\n",
    "\n",
    "    num_scc = len(scc)\n",
    "    largest_scc = max((len(c) for c in scc), default=0)\n",
    "\n",
    "    num_wcc = len(wcc)\n",
    "    largest_wcc = max((len(c) for c in wcc), default=0)\n",
    "\n",
    "    # --- Density ---\n",
    "    density = graph.density(loops=False)\n",
    "\n",
    "    # --- Reciprocity ---\n",
    "    reciprocity = graph.reciprocity()\n",
    "\n",
    "    # --- Return as DataFrame ---\n",
    "    stats = {\n",
    "        \"num_vertices\": n,\n",
    "        \"num_edges\": m,\n",
    "        \"num_scc\": num_scc,\n",
    "        \"largest_scc\": largest_scc,\n",
    "        \"num_wcc\": num_wcc,\n",
    "        \"largest_wcc\": largest_wcc,\n",
    "        \"density\": density,\n",
    "        \"reciprocity\": reciprocity,\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame([stats])\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "# Example after loading:\n",
    "epinion_stats = basic_stats(raw_epinion_graph)\n",
    "print(\"EPINION STATS\")\n",
    "display(epinion_stats)\n",
    "\n",
    "\n",
    "#TESTS\n",
    "def test_basic_stats():\n",
    "    # Create a small directed graph\n",
    "    # 0 -> 1 -> 2 -> 0 forms one SCC of size 3\n",
    "    # 3 -> 4 is a separate weak component\n",
    "    edges = [(0, 1), (1, 2), (2, 0), (3, 4)]\n",
    "    G = ig.Graph(edges=edges, directed=True)\n",
    "\n",
    "    # Run basic stats\n",
    "    df = basic_stats(G)\n",
    "\n",
    "    # Assertions\n",
    "    assert df[\"num_vertices\"][0] == 5\n",
    "    assert df[\"num_edges\"][0] == 4\n",
    "\n",
    "    assert df[\"num_scc\"][0] == 3   # {0,1,2}, {3}, {4}\n",
    "    assert df[\"largest_scc\"][0] == 3\n",
    "\n",
    "    assert df[\"num_wcc\"][0] == 2   # {0,1,2} and {3,4}\n",
    "    assert df[\"largest_wcc\"][0] == 3\n",
    "\n",
    "    print(\"✅ test_basic_stats passed!\")\n",
    "\n",
    "test_basic_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above we see that we have a relatively sparse graph since the total number of edges is much less than the total possible number of edges given the number of vertices.  \n",
    "Namely, there are $$75879 \\cdot 75878 \\approx 5.7 \\cdot 10^9$$ possible edges however there are only $$5 \\cdot 10^5$$ actual edges in this graph.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3N_jJm8jH_Ie"
   },
   "source": [
    "# 4. Quick Visualization (Exploratory)\n",
    "Produce a small subgraph visualization to build intuition (e.g., induced subgraph of top-`k` PageRank nodes or a random 500-node sample). For large graphs, you **don't have to** try to plot everything.\n",
    "\n",
    "- Annotate what you observe (hubs? communities? sources/sinks?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W_j0IsZGIBKv"
   },
   "outputs": [],
   "source": [
    "def visualize_subgraph(G):\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fe4gU7_huyiA"
   },
   "source": [
    "# 5.\n",
    "\n",
    "Prepare a concise research brief summarizing your data mining results and their significance.\n",
    "\n",
    "Your write-up must include:\n",
    "\n",
    "1. Results\n",
    "- Clearly and precisely report your empirical findings.\n",
    "- Include relevant quantitative results (e.g., metrics, comparisons, trends, error analysis).\n",
    "- Figures or tables may be used if they improve clarity, but they must be referenced and interpreted in the text.\n",
    "- You can use markdown formatting (bold, italics, headings, etc.) to help you communicate your findings.\n",
    "- Do not focus on implementation details unless they are necessary to understand the results.\n",
    "\n",
    "2. Significance and Interpretation\n",
    "- Explain why the results matter in a data mining context.\n",
    "- Discuss what the findings imply about the data, the model(s), or the assumptions made.\n",
    "- Address limitations, tradeoffs, or unexpected outcomes where relevant.\n",
    "\n",
    "Clearly state what new insight is gained from your results.\n",
    "Aim for 5-7 paragraphs in length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "On my honor, I declare the following resources:\n",
    "1. Collaborators:\n",
    "- \n",
    "\n",
    "2. Web Sources:\n",
    "- Jure Leskovec and Andrej Krevl. SNAP Datasets: Stanford Large Network Dataset Collection. http://snap.stanford.edu/data. (used for searching for relevant datasets)\n",
    "- https://snap.stanford.edu/data/soc-Epinions1.txt.gz - dataset used for analysis\n",
    "- https://python.igraph.org/en/stable/ - documentation for igraph, used to find out how to use igraph (igraph API docs)\n",
    "\n",
    "3. AI Tools:\n",
    "- ChatGPT: Wrote the test cases for part 1, the generated code worked, i did not have to change it. \n",
    "- ChatGPT: Wrote the test cases for part 2, the generated code worked, and i did not have to change it. Also wrote the gunzip function, for which it also provided functional code. Also generated the vast majority of the load_directed_graph function, I just provided it with which parameters the function should accept and a high level overview of what the function should do.\n",
    "- ChatGPT: Wrote the test cases for part 3, the generated code worked, and i did not have to change it. For the basic stats function, I just gave it the parameters and expected behavior and it filled in the rest, didn't have to modify the code.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5cqkoNzB27p"
   },
   "source": [
    "# B [24pts]. Interview Questions\n",
    "\n",
    "We now pretend this is a real job interview. Here's some guidance on how to answer these questions:\n",
    "\n",
    "1. Briefly restate the question and state any assumptions you are making.\n",
    "\n",
    "2. Explain your reasoning out loud, focusing on tradeoffs, limitations, and constraints.\n",
    "\n",
    "3. As a principle, keep your answers as short and clear as they can be (while still answering the question).\n",
    "\n",
    "4. Write/speak in a conversational but professional tone (avoid being overly formal). For speaking: speak at a reasonable pace and volume, speak clearly, pause when you need to, and practice making \"eye contact\" with the camera. Keep a confident, positive, and professional tone. *For additional coaching and practice, the University Writing Center provides individual appointments: https://writingcenter.tamu.edu/make-an-appointment.*\n",
    "\n",
    "There may not be a single correct answer. We are grading whether your reasoning is reasonable and aware of limitations.\n",
    "\n",
    "\n",
    "**Rubric**\n",
    "\n",
    "[8pt] Clear understanding of the question; reasonable assumptions; thoughtful reasoning that acknowledges tradeoffs and limitations; clear, concise communication in a conversational but professional tone (for speaking: clear pace, volume, and articulation).\n",
    "\n",
    "[4pt] Basic understanding but shallow reasoning or unclear assumptions; communication is somewhat unclear, overly verbose, or overly informal/formal.\n",
    "\n",
    "[0pt] Minimal, unclear, or incorrect response; poor communication or unprofessional tone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kn4FqXDlByUC"
   },
   "source": [
    "# 1.\n",
    "Many real systems can be represented as graphs in multiple ways. How would you decide what the nodes and edges should represent in a given domain, and what kinds of errors can arise from a poor abstraction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eiv9Di4CKL5Y"
   },
   "source": [
    "# 2.\n",
    "Discuss how missing edges, spurious edges, or sampling bias affect centrality-based conclusions. Which measures are most fragile?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZlnNs746c-J"
   },
   "source": [
    "# 3.\n",
    "As a video (reminder to keep it brief, 2 minutes max): So, I see you did a graph analysis (referring to this homework). That's cool -- can you walk me through what you did?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BOimHuE4H7D"
   },
   "source": [
    "# C [4pts]. What new questions do you have?\n",
    "We want you to think bigger! Tell us what questions and curiosity this homework brings up for you.\n",
    "\n",
    "**Rubric**\n",
    "\n",
    "[4pt] Complete, thoughtful response.\n",
    "\n",
    "[2pt] Partial response.\n",
    "\n",
    "[0pt] Minimal response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1FQ3Cys4J2P"
   },
   "source": [
    "# 1.\n",
    "What new questions do you have about association rule mining (in general) after this homework? Or, what topics are you curious about now? List at least 3."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ak3SqhmRDYKW"
   },
   "source": [
    "## CSCE 676 :: Data Mining and Analysis :: Texas A&M University :: Spring 2026\n",
    "\n",
    "\n",
    "# Weekly Homework 6: Clustering\n",
    "\n",
    "\n",
    "***Goals of this homework:***\n",
    "* Understand why clustering is used in unsupervised learning.\n",
    "* Apply and compare k-means and DBSCAN.\n",
    "* Recognize strengths/weaknesses of each method based on data shape, density, and noise.\n",
    "* Choose parameters appropriately.\n",
    "* Evaluate clustering quality using quantitative metrics rather than visuals alone.\n",
    "\n",
    "\n",
    "***Submission instructions:***\n",
    "\n",
    "You should post your notebook to Canvas (look for the assignment there). Please name your submission **your-uin_hw6.ipynb**, so for example, my submission would be something like **555001234_hw6.ipynb**. Your notebook should be fully executed when you submit ... so run all the cells for us so we can see the output, then submit that.\n",
    "\n",
    "***Grading philosophy:***\n",
    "\n",
    "We are grading reasoning, judgment, and clarity, not just correctness. Show us that you understand the data, the constraints, and the limits of your conclusions.\n",
    "\n",
    "***For each question, you need to respond with 2 cells:***\n",
    "1. **[A Code Cell] Your Code:**\n",
    "  - If code is not applicable for the question, you can skip this cell.\n",
    "  - For tests: tests can be simple assertions or checks (e.g., using `assert` or `print` or small functions or visual inspection); formal testing frameworks are not required.\n",
    "2. **[A Markdown Cell] Your Answer:** Write up your answers and explain them in complete sentences. Include any videos in this section as well; for videos, upload them to your TAMU Google Drive, and ensure they are set to be visible by the instruction team (set to: **anyone with a TAMU email can view**), then share the link to the video in the cell.\n",
    "\n",
    "***At the end of each Section (A/B/C/...) include a cell for your resources:***\n",
    "\n",
    "**[A Markdown Cell] Your Resources:** You need to cite 3 types of resources and note how they helped you: (1) Collaborators, (2) Web Sources (e.g. StackOverflow), and (3) AI Tools (you must also describe how you prompted, but we do not require any links to any specific chats). Specifically, use the following format as a template:\n",
    "```\n",
    "On my honor, I declare the following resources:\n",
    "1. Collaborators:\n",
    "- Reveille A.: Helped me understand that a df in pandas is a data structure kinda like a CSV.\n",
    "- Sully A.: Helped me fix a bug with the vector addition of 2 columns.\n",
    "- ...\n",
    "\n",
    "2. Web Sources:\n",
    "- https://stackoverflow.com/questions/46562479/python-pandas-data-frame-creation: how to create a pd df\n",
    "- ...\n",
    "\n",
    "3. AI Tools:\n",
    "- ChatGPT: I gave it the homework .ipynb file and the ufo.csv, and told it to generate the code for the first question, but it did it with csv.reader(), so I re-prompted it to use pandas and that one was correct\n",
    "- ...\n",
    "```\n",
    "***Why do we require this cell?*** This cell is important...\n",
    "\n",
    "1. For academic integrity, you must give credit where credit is due.\n",
    "\n",
    "2. We want you to pay attention to how you can successfully get help to move through problems! Is there someone you work with or an AI tool that helps you learn the material better? That's great! The point of engineering is to use your tools to solve hard problems, and part of graduate school is learning about how *you* learn and solve problems best.\n",
    "\n",
    "***A reminder: you get out of it what you put into it.***\n",
    "Do your best on these homeworks, show us your creativity, and ask for help when you need it -- good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adEgfyRL-Bkk"
   },
   "source": [
    "# A [60pts]. Decision Trees\n",
    "\n",
    "**Rubric**\n",
    "\n",
    "[10 pts] Strong/Professional: Correct and complete implementation of the task; Reasonable assumptions, stated or implied, and justified; Thoughtful handling of real-world data issues (missingness, noise, scale, duplicates, edge cases); Clear, concise explanations of what was done and why; Code is clean, readable, and well-structured, uses appropriate pandas, and would plausibly pass a professional code review; Tests meaningfully validate non-trivial behavior (not just \"the code runs so it must be right\").\n",
    "\n",
    "[5 pts] Partial/Developing: Core task mostly completed but with gaps, weak assumptions, or minor mistakes; Reasoning is shallow or mostly descriptive; Code works but is messy, repetitive, or fragile; Tests are superficial, incomplete, or poorly motivated.\n",
    "\n",
    "[0 pts] Minimal/Incorrect: Task is largely incorrect, missing, or misunderstands the goal; Little to no reasoning or justification; Code does not run or ignores constraints; No meaningful tests.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQQQGCzpTa9Z"
   },
   "source": [
    "## Environment Setup & Sampling (Optional)\n",
    "\n",
    "- You may use the full datasets. Sampling is optional (for speed).\n",
    "- If you sample, briefly report what you did (n/frac, whether you stratified, any seed).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kqHj1Eu8UcUb"
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "Jeopardy_data_URL ='https://www.kaggle.com/datasets/tunguz/200000-jeopardy-questions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DG0wkF_2Tid1",
    "outputId": "08888aba-6324-4e8c-a3bd-ccc26eeb84cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jeopardy: (216930, 7) -> sample: (216930, 7)\n"
     ]
    }
   ],
   "source": [
    "###### sampling code (optional)\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Edit paths if needed\n",
    "JEOPARDY_PATH = Path(\"JEOPARDY_CSV-2.csv\")\n",
    "\n",
    "def load_csv(path, **kwargs):\n",
    "    if path.exists():\n",
    "        return pd.read_csv(path, **kwargs)\n",
    "    print(f\"Warning: {path} not found.\")\n",
    "    return None\n",
    "\n",
    "jeopardy = load_csv(JEOPARDY_PATH)\n",
    "\n",
    "# ====== (Optional) Sampling ======\n",
    "# Leave all values as None to use the full dataset.\n",
    "SAMPLE = {\n",
    "    \"jeopardy\": {\"n\": None, \"frac\": None, \"random_state\": None, \"stratify_col\": None},  # e.g., {\"n\": 20000, \"random_state\": 42}\n",
    "}\n",
    "\n",
    "def maybe_sample(df, cfg):\n",
    "    \"\"\"Return sampled df if n/frac set; otherwise return df. Optional stratify by a column name.\"\"\"\n",
    "    if df is None:\n",
    "        return None\n",
    "    n, frac, rs, strat = cfg.get(\"n\"), cfg.get(\"frac\"), cfg.get(\"random_state\"), cfg.get(\"stratify_col\")\n",
    "    if strat and strat in df.columns and (n or frac):\n",
    "        # stratified sampling (simple & proportional when using frac)\n",
    "        if frac:\n",
    "            return (df.groupby(strat, group_keys=False)\n",
    "                      .apply(lambda g: g.sample(frac=frac, random_state=rs))\n",
    "                      .reset_index(drop=True))\n",
    "        # proportional n by class frequency (rounded)\n",
    "        counts = df[strat].value_counts(normalize=True) * n\n",
    "        parts = []\n",
    "        for k, need in counts.round().astype(int).items():\n",
    "            part = df[df[strat]==k].sample(n=min(need, len(df[df[strat]==k])), random_state=rs)\n",
    "            parts.append(part)\n",
    "        out = pd.concat(parts).reset_index(drop=True)\n",
    "        return out.sample(frac=1.0, random_state=rs).reset_index(drop=True)\n",
    "    # simple sampling\n",
    "    if frac: return df.sample(frac=frac, random_state=rs).reset_index(drop=True)\n",
    "    if n:    return df.sample(n=min(n, len(df)), random_state=rs).reset_index(drop=True)\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "jeopardy_sample = maybe_sample(jeopardy, SAMPLE[\"jeopardy\"])\n",
    "\n",
    "print(\"Jeopardy:\", None if jeopardy is None else jeopardy.shape,\n",
    "      \"-> sample:\", None if jeopardy_sample is None else jeopardy_sample.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "awhpaw2cTskO"
   },
   "source": [
    "# 1. Text Cleaning\n",
    "\n",
    "_Jeopardy!_ is a popular U.S. quiz show where contestants are given answers and must respond with the questions.\n",
    "> For example: Answer: “This planet is known as the Red Planet.” Correct question: “What is Mars?”\n",
    "\n",
    "In this dataset, each question has a Category (topic), Question (text), and Answer (label).\n",
    "You will analyze patterns in these texts and explore how question wording relates to categories and answers.\n",
    "\n",
    "- Clean the text (e.g., lowercase, remove stopwords/punctuation, optional stemming or lemmatization). You may use any pipeline you like, including optional TF-IDF features, embeddings, or whatever you like. Many packages (e.g., sklearn) come built in with some nice text processing packages.\n",
    "\n",
    "- Write at least 2 tests for your code (focus on the most complicated parts), then answer: What did you test for? How do you know your code is correct?\n",
    "\n",
    "- Briefly explain your design choices (what steps or parameters you used) and give a short rationale (why you think these choices help).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/keegan-\n",
      "[nltk_data]     smith/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/keegan-\n",
      "[nltk_data]     smith/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Show Number    Air Date      Round               Category Value  \\\n",
      "0         4680  2004-12-31  Jeopardy!                history  $200   \n",
      "1         4680  2004-12-31  Jeopardy!  espns alltime athlete  $200   \n",
      "2         4680  2004-12-31  Jeopardy!         everybody talk  $200   \n",
      "3         4680  2004-12-31  Jeopardy!           company line  $200   \n",
      "4         4680  2004-12-31  Jeopardy!        epitaph tribute  $200   \n",
      "\n",
      "                                            Question      Answer  \n",
      "0  year life galileo house arrest espousing man t...  Copernicus  \n",
      "1  olympian football star carlisle indian school ...  Jim Thorpe  \n",
      "2  city yuma state record average hour sunshine year     Arizona  \n",
      "3  live art linkletter company served billionth b...  McDonald's  \n",
      "4  signer dec indep framer constitution mass seco...  John Adams  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Remove punctuation\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # 3. Tokenize\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # 4. Remove stopwords\n",
    "    tokens = [t for t in tokens if t not in ENGLISH_STOP_WORDS]\n",
    "    \n",
    "    # 5. Lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    \n",
    "    # 6. Rejoin\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def preprocess_jeopardy(df):\n",
    "    df.columns = df.columns.str.strip()\n",
    "    df[\"Question\"] = df[\"Question\"].apply(clean_text)\n",
    "    df[\"Category\"] = df[\"Category\"].apply(clean_text)\n",
    "    df[\"Answer\"] = df[\"Answer\"].apply(clean_text)\n",
    "    return df\n",
    "\n",
    "\n",
    "def problem1():\n",
    "    preprocess_jeopardy(jeopardy_sample)\n",
    "\n",
    "problem1()\n",
    "\n",
    "def test_punctuation_and_lowercase():\n",
    "    input_text = \"This Planet is Known as the Red Planet!\"\n",
    "    cleaned = clean_text(input_text)\n",
    "    assert \"!\" not in cleaned\n",
    "    assert cleaned == cleaned.lower()\n",
    "    \n",
    "def test_stopwords_and_lemmatization():\n",
    "    input_text = \"Cats are running in the gardens\"\n",
    "    cleaned = clean_text(input_text)\n",
    "    expected_words = [\"cat\", \"running\", \"garden\"]  # after stopword removal\n",
    "    for word in expected_words:\n",
    "        assert word in cleaned\n",
    "    assert \"the\" not in cleaned\n",
    "    assert \"are\" not in cleaned\n",
    "\n",
    "test_punctuation_and_lowercase()\n",
    "test_stopwords_and_lemmatization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tested for removal of punctuation and for the removal of stop words and lemmatization. I know my code is likely correct because these two random strings were successfully processed.  \n",
    "I performed very basic string processing: converted to lowercase to ensure a word like aArDvArk == aardvark, e.g the case of a word usually doesn't have much to do with its semantic meaning. I also removed any characters which weren't a-z as they too should not change the semantic meaning of the text. I then removed Stop words which are high frequency words that don't hold a lot of semantic weight, like \"the\". Finally I did lemmatization, which is essentially converting synonyms to the same base word. E.g better becomes \"good\". I think these choices help because "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gc_xhR_MXWXa"
   },
   "source": [
    "***These next few parts are designed so you can observe the challenges of evaluating clustering.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJDbzG45TsmQ"
   },
   "source": [
    "# 2. Evaluating **K-Means** with Metrics\n",
    "\n",
    "- Run K-Means.\n",
    "- Report some clustering metrics to assess the quality of clustering (you may consider the Silhouette score for each method, or perhaps `ARI` and `NMI` using the Category labels as ground truth).\n",
    "- For each metric, you should try a few parameter values to determine the best values for the dataset. Plot your results, and include: what value are you varying & what value are you measuring to estimate cluster quality?\n",
    "- Write about: What is the best value of $k$ based on your data? Why? (Use your results to back up your answers.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lYuSdkkCW7dP"
   },
   "source": [
    "# 2. Evaluating **DBSCAN** with Metrics\n",
    "\n",
    "- Run DBSCAN.\n",
    "- Report some clustering metrics to assess the quality of clustering (you may consider the Silhouette score for each method, or perhaps `ARI` and `NMI` using the Category labels as ground truth).\n",
    "- For each metric, you should try a few parameter values to determine the best values for the dataset. Plot your results, and include: what value are you varying & what value are you measuring to estimate cluster quality?\n",
    "- Write about: What are the best values based on your data? Why? (Use your results to back up your answers.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tG12V_o4W7Q2"
   },
   "source": [
    "# 3. Comparing K-Means and DBSCAN **with Metrics**\n",
    "\n",
    "- Compare your K-Means and DBSCAN results, and defend: If you had to choose one for this dataset, which one would you choose? Why? (Use your results to back up your answer.)\n",
    "- The labels (categories) in this dataset are very sparse; however, your data has a useful property: e.g., categories might be 1800s British Literature, British Writers, British Lit --> those are three different labels even though they may conceptually be the same thing. Think about and answer: how could you exploit this property in your evaluation setup?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kIjdqu-TW7Wd"
   },
   "source": [
    "# 4. Comparing K-Means and DBSCAN **Visually with PCA**\n",
    "\n",
    "- For K-Means and DBSCAN, visualize the clusters in 2D with PCA\n",
    "- Try a few parameter values to determine the best values for the dataset. Plot your results, and include: what value are you varying & what do you see in the plot (+ what does this lead you to conclude about cluster quality based on the visualization)?\n",
    "- Write about: What are the best values based on your data? Why? (Use your results to back up your answers.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d1OgusgYYJ-C"
   },
   "source": [
    "# 5. Comparing K-Means and DBSCAN **Visually with t-SNE**\n",
    "\n",
    "- For K-Means and DBSCAN, visualize the clusters in 2D with t-SNE\n",
    "- Try a few parameter values to determine the best values for the dataset. Plot your results, and include: what value are you varying & what do you see in the plot (+ what does this lead you to conclude about cluster quality based on the visualization)?\n",
    "- Write about: What are the best values based on your data? Why? (Use your results to back up your answers.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnqMZ0-jYaos"
   },
   "source": [
    "# 6. Comparing K-Means and DBSCAN **with Visualization**\n",
    "- Compare your K-Means and DBSCAN results, and defend: If you had to choose one for this dataset, which one would you choose? Why? (Use your results to back up your answer.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5cqkoNzB27p"
   },
   "source": [
    "# B [32pts]. Interview Questions\n",
    "\n",
    "We now pretend this is a real job interview. Here's some guidance on how to answer these questions:\n",
    "\n",
    "1. Briefly restate the question and state any assumptions you are making.\n",
    "\n",
    "2. Explain your reasoning out loud, focusing on tradeoffs, limitations, and constraints.\n",
    "\n",
    "3. As a principle, keep your answers as short and clear as they can be (while still answering the question).\n",
    "\n",
    "4. Write/speak in a conversational but professional tone (avoid being overly formal). For speaking: speak at a reasonable pace and volume, speak clearly, pause when you need to, and practice making \"eye contact\" with the camera. Keep a confident, positive, and professional tone. *For additional coaching and practice, the University Writing Center provides individual appointments: https://writingcenter.tamu.edu/make-an-appointment.*\n",
    "\n",
    "There may not be a single correct answer. We are grading whether your reasoning is reasonable and aware of limitations.\n",
    "\n",
    "\n",
    "**Rubric**\n",
    "\n",
    "[4pt] Clear understanding of the question; reasonable assumptions; thoughtful reasoning that acknowledges tradeoffs and limitations; clear, concise communication in a conversational but professional tone (for speaking: clear pace, volume, and articulation).\n",
    "\n",
    "[2pt] Basic understanding but shallow reasoning or unclear assumptions; communication is somewhat unclear, overly verbose, or overly informal/formal.\n",
    "\n",
    "[0pt] Minimal, unclear, or incorrect response; poor communication or unprofessional tone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kn4FqXDlByUC"
   },
   "source": [
    "# 1.\n",
    "How many metrics do you need to use to justify your clustering approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QlrsnjJ8VakG"
   },
   "source": [
    "# 2.\n",
    "What does it mean for clusters to be “real” if different algorithms produce different partitions of the same dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Wzr9V9HVamW"
   },
   "source": [
    "# 3.\n",
    "When is the question “how many clusters exist?” fundamentally ill-posed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68h49g8gVaou"
   },
   "source": [
    "# 4.\n",
    "Explain how every clustering algorithm encodes an implicit definition of similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3L1LxpavliBR"
   },
   "source": [
    "#5.\n",
    "What metrics would you monitor in production to detect clustering drift?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npttkRpfliKX"
   },
   "source": [
    "# 6.\n",
    "Design a system that automatically groups incoming customer support tickets into topics in real time. What components exist from ingestion → preprocessing → embedding → clustering → serving?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcbRaSHul69p"
   },
   "source": [
    "# 7.\n",
    "You have limited memory but a very large dataset — how do you cluster it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "myKMJgnqVa-9"
   },
   "source": [
    "# 8.\n",
    "What happens if data distribution shifts drastically (e.g., breaking news topic spike)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7J6f-8G5baeK"
   },
   "source": [
    "# 9.\n",
    "(Video; 1 minute max) What happens if data distribution shifts drastically (e.g., breaking news topic spike)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BOimHuE4H7D"
   },
   "source": [
    "# C [4pts]. What new questions do you have?\n",
    "We want you to think bigger! Tell us what questions and curiosity this homework brings up for you.\n",
    "\n",
    "**Rubric**\n",
    "\n",
    "[4pt] Complete, thoughtful response.\n",
    "\n",
    "[2pt] Partial response.\n",
    "\n",
    "[0pt] Minimal response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1FQ3Cys4J2P"
   },
   "source": [
    "# 1.\n",
    "What new questions do you have after this homework? Or, what topics are you curious about now? List at least 3."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

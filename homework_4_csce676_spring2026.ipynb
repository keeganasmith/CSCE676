{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ak3SqhmRDYKW"
   },
   "source": [
    "## CSCE 676 :: Data Mining and Analysis :: Texas A&M University :: Spring 2026\n",
    "\n",
    "\n",
    "# Weekly Homework 4: Graphs! Part 2\n",
    "\n",
    "\n",
    "***Goals of this homework:***\n",
    "Perform an analysis of a graph of your choice.\n",
    "\n",
    "\n",
    "***Submission instructions:***\n",
    "\n",
    "You should post your notebook to Canvas (look for the assignment there). Please name your submission **your-uin_hw3.ipynb**, so for example, my submission would be something like **555001234_hw3.ipynb**. Your notebook should be fully executed when you submit ... so run all the cells for us so we can see the output, then submit that.\n",
    "\n",
    "***Grading philosophy:***\n",
    "\n",
    "We are grading reasoning, judgment, and clarity, not just correctness. Show us that you understand the data, the constraints, and the limits of your conclusions.\n",
    "\n",
    "***For each question, you need to respond with 2 cells:***\n",
    "1. **[A Code Cell] Your Code:**\n",
    "  - If code is not applicable for the question, you can skip this cell.\n",
    "  - For tests: tests can be simple assertions or checks (e.g., using `assert` or `print` or small functions or visual inspection); formal testing frameworks are not required.\n",
    "2. **[A Markdown Cell] Your Answer:** Write up your answers and explain them in complete sentences. Include any videos in this section as well; for videos, upload them to your TAMU Google Drive, and ensure they are set to be visible by the instruction team (set to: **anyone with a TAMU email can view**), then share the link to the video in the cell.\n",
    "\n",
    "***At the end of each Section (A/B/C/...) include a cell for your resources:***\n",
    "\n",
    "**[A Markdown Cell] Your Resources:** You need to cite 3 types of resources and note how they helped you: (1) Collaborators, (2) Web Sources (e.g. StackOverflow), and (3) AI Tools (you must also describe how you prompted, but we do not require any links to any specific chats). Specifically, use the following format as a template:\n",
    "```\n",
    "On my honor, I declare the following resources:\n",
    "1. Collaborators:\n",
    "- Reveille A.: Helped me understand that a df in pandas is a data structure kinda like a CSV.\n",
    "- Sully A.: Helped me fix a bug with the vector addition of 2 columns.\n",
    "- ...\n",
    "\n",
    "2. Web Sources:\n",
    "- https://stackoverflow.com/questions/46562479/python-pandas-data-frame-creation: how to create a pd df\n",
    "- ...\n",
    "\n",
    "3. AI Tools:\n",
    "- ChatGPT: I gave it the homework .ipynb file and the ufo.csv, and told it to generate the code for the first question, but it did it with csv.reader(), so I re-prompted it to use pandas and that one was correct\n",
    "- ...\n",
    "```\n",
    "***Why do we require this cell?*** This cell is important...\n",
    "\n",
    "1. For academic integrity, you must give credit where credit is due.\n",
    "\n",
    "2. We want you to pay attention to how you can successfully get help to move through problems! Is there someone you work with or an AI tool that helps you learn the material better? That's great! The point of engineering is to use your tools to solve hard problems, and part of graduate school is learning about how *you* learn and solve problems best.\n",
    "\n",
    "***A reminder: you get out of it what you put into it.***\n",
    "Do your best on these homeworks, show us your creativity, and ask for help when you need it -- good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adEgfyRL-Bkk"
   },
   "source": [
    "# A [72pts]. Step-by-Step Data Mining & Experimental Analysis on A Graph of Your Choice -- A Continuation of Last Week!\n",
    "\n",
    "**Rubric**\n",
    "\n",
    "[18 pts] Strong/Professional: Correct and complete implementation of the task; Reasonable assumptions, stated or implied, and justified; Thoughtful handling of real-world data issues (missingness, noise, scale, duplicates, edge cases); Clear, concise explanations of what was done and why; Code is clean, readable, and well-structured, uses appropriate pandas, and would plausibly pass a professional code review; Tests meaningfully validate non-trivial behavior (not just \"the code runs so it must be right\").\n",
    "\n",
    "[9 pts] Partial/Developing: Core task mostly completed but with gaps, weak assumptions, or minor mistakes; Reasoning is shallow or mostly descriptive; Code works but is messy, repetitive, or fragile; Tests are superficial, incomplete, or poorly motivated.\n",
    "\n",
    "[0 pts] Minimal/Incorrect: Task is largely incorrect, missing, or misunderstands the goal; Little to no reasoning or justification; Code does not run or ignores constraints; No meaningful tests.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILBA0EepHf5g"
   },
   "source": [
    "## Overview\n",
    "In this homework, you will **choose one dataset you like** from the [SNAP datasets](https://snap.stanford.edu/data/index.html) collection from the **Social networks** section. You must choose a  **directed graph only**. In this section, you will perform a step‑by‑step data mining & experimental analysis. *Much of this section is self-directed, meaning you will need to make critical decisions about what tools you use and what you explore.*\n",
    "\n",
    "Ideally, you should eventually turn in a coherent story: **What you tried → Why → What you found → So what? → Wait...Anything more?**. It's completely OKAY if you only get minor discoveries. But you should always document the whole learning and reasoning process. Grading will be based on the logic and coherence of your submitted notebook.\n",
    "\n",
    "As a guide, for each step of the homework, you should briefly document:\n",
    "- **Method choice & rationale.** Why this method? What do you expect?\n",
    "- **Parameters.** E.g., `alpha=0.85` for PageRank; seed selection for PPR; thresholds for community extraction.\n",
    "- **Results.** Tables/plots + **1–3 sentences** of interpretation.\n",
    "- **Reflection.** Did results match your expectations? If not, why might that be?\n",
    "\n",
    "Ideally, strong submissions should read like a *short research memo* rather than a raw dump of numbers.\n",
    "\n",
    "## Environment Setup\n",
    "You may use **Python 3.9+**. Other possible tools are:\n",
    "- **Graph tools**: You are welcome to use existing tools that are optimised for graph learning. Among them, `networkx`is recommended. `graph-tool` and `igraph` are also good when dealing with larger graphs.\n",
    "- **Data processing packages**: eg. `numpy`, `pandas`\n",
    "- **Plotting**: `matplotlib`, `plotly`, `seaborn` etc.\n",
    "- **other related tools** `scipy`, `scikit-learn`,  `pytorch` etc.\n",
    "- You may choose other tools; just be sure to let us know.\n",
    "\n",
    "> If your chosen dataset is very large, consider using `graph-tool` and `igraph`, or sampling/induced subgraphs to stay within reasonable time/memory limits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Bxs0anrICm3"
   },
   "source": [
    "# 1. Centrality Analyses\n",
    "First, you must compute\n",
    "- **PageRank**\n",
    "as a baseline centrality measure.\n",
    "\n",
    "Then you should choose at least one of the following to compare with:\n",
    "\n",
    "- **HITS** (hubs/authorities)\n",
    "- **In-degree** and **out-degree** centrality (as baselines)\n",
    "- **Katz** or **eigenvector** centrality on a symmetrized version\n",
    "\n",
    "Create a small table of **top-20 nodes** by each measure and comment on overlaps/differences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[download] Fetching: https://snap.stanford.edu/data/soc-Slashdot0902.txt.gz\n",
      "[download] Saved to: ../data/soc-Slashdot0902.txt.gz (3,246,518 bytes)\n",
      "[load] Reading edges...\n",
      "[load] Raw edges read: 948,464\n",
      "[graph] Building igraph directed graph...\n",
      "[graph] Vertices: 82,168  Edges: 870,161  (after simplify)\n",
      "[compute] PageRank...\n",
      "[compute] HITS hub/authority scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/99/q6jxjyj9617_wy2l0435c4r00000gn/T/ipykernel_53672/2081169850.py:112: RuntimeWarning: More than 30% of hub or authority scores are zeros. The presence of zero values indicates that the solution is not unique, thus the returned result may not be meaningful. Location: src/centrality/hub_authority.c:81\n",
      "  hubs = g.hub_score()          # directed by default\n",
      "/var/folders/99/q6jxjyj9617_wy2l0435c4r00000gn/T/ipykernel_53672/2081169850.py:113: RuntimeWarning: More than 30% of hub or authority scores are zeros. The presence of zero values indicates that the solution is not unique, thus the returned result may not be meaningful. Location: src/centrality/hub_authority.c:81\n",
      "  auth = g.authority_score()    # directed by default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[compute] In-degree / Out-degree...\n",
      "\n",
      "=== Top-20 tables (by measure) ===\n",
      "\n",
      "--- PageRank ---\n",
      "measure     rank       node      score      \n",
      "PageRank    1          2494       0.00273194\n",
      "PageRank    2           398       0.00252651\n",
      "PageRank    3           381       0.00236492\n",
      "PageRank    4          4805       0.00181652\n",
      "PageRank    5            37       0.00181183\n",
      "PageRank    6           226       0.00152338\n",
      "PageRank    7          5706       0.00137247\n",
      "PageRank    8          4826       0.00110501\n",
      "PageRank    9           219       0.00100132\n",
      "PageRank   10          5057      0.000895077\n",
      "PageRank   11          5513      0.000859242\n",
      "PageRank   12          2828      0.000850472\n",
      "PageRank   13          3482      0.000849135\n",
      "PageRank   14          9190      0.000797863\n",
      "PageRank   15            49      0.000791423\n",
      "PageRank   16          5383      0.000735466\n",
      "PageRank   17         13004       0.00073495\n",
      "PageRank   18          2483      0.000728981\n",
      "PageRank   19          5685      0.000674163\n",
      "PageRank   20           217      0.000658543\n",
      "\n",
      "--- HITS_hub ---\n",
      "measure     rank       node      score     \n",
      "HITS_hub    1            49             1  \n",
      "HITS_hub    2          2494      0.937153  \n",
      "HITS_hub    3           398      0.932676  \n",
      "HITS_hub    4          4805      0.922554  \n",
      "HITS_hub    5           195      0.900765  \n",
      "HITS_hub    6           342      0.830956  \n",
      "HITS_hub    7          1723      0.827236  \n",
      "HITS_hub    8          5453      0.775395  \n",
      "HITS_hub    9          2542       0.72155  \n",
      "HITS_hub   10          1509      0.714753  \n",
      "HITS_hub   11          2500      0.710111  \n",
      "HITS_hub   12          2488       0.70214  \n",
      "HITS_hub   13           324      0.697915  \n",
      "HITS_hub   14          5504      0.670447  \n",
      "HITS_hub   15         32098      0.663497  \n",
      "HITS_hub   16          4823      0.662144  \n",
      "HITS_hub   17           413      0.660396  \n",
      "HITS_hub   18          2491      0.658154  \n",
      "HITS_hub   19          5613      0.647074  \n",
      "HITS_hub   20          2960      0.645646  \n",
      "\n",
      "--- HITS_authority ---\n",
      "measure         rank       node      score     \n",
      "HITS_authority  1         4805              1  \n",
      "HITS_authority  2          398       0.984375  \n",
      "HITS_authority  3           49       0.973969  \n",
      "HITS_authority  4         2494       0.958211  \n",
      "HITS_authority  5          195       0.817286  \n",
      "HITS_authority  6          342       0.769687  \n",
      "HITS_authority  7         1723       0.745889  \n",
      "HITS_authority  8         5453       0.698754  \n",
      "HITS_authority  9         2488       0.668003  \n",
      "HITS_authority 10         2542       0.667333  \n",
      "HITS_authority 11         2500       0.658612  \n",
      "HITS_authority 12         1509        0.64989  \n",
      "HITS_authority 13          324       0.644194  \n",
      "HITS_authority 14         4823       0.622232  \n",
      "HITS_authority 15         5504       0.617033  \n",
      "HITS_authority 16         2491       0.615766  \n",
      "HITS_authority 17         4828       0.609294  \n",
      "HITS_authority 18         5390         0.6019  \n",
      "HITS_authority 19         3482       0.600613  \n",
      "HITS_authority 20          413       0.598025  \n",
      "\n",
      "--- InDegree ---\n",
      "measure     rank       node      score     \n",
      "InDegree    1         2494       2552      \n",
      "InDegree    2          398       2354      \n",
      "InDegree    3         4805       2291      \n",
      "InDegree    4          381       1861      \n",
      "InDegree    5          226       1728      \n",
      "InDegree    6           37       1660      \n",
      "InDegree    7         4826       1270      \n",
      "InDegree    8         5706       1110      \n",
      "InDegree    9         5057       1102      \n",
      "InDegree   10          219       1050      \n",
      "InDegree   11         5513       1024      \n",
      "InDegree   12         5383        994      \n",
      "InDegree   13         9190        972      \n",
      "InDegree   14           49        962      \n",
      "InDegree   15         3482        959      \n",
      "InDegree   16         2483        901      \n",
      "InDegree   17          217        847      \n",
      "InDegree   18         5685        799      \n",
      "InDegree   19         3483        789      \n",
      "InDegree   20         2828        737      \n",
      "\n",
      "--- OutDegree ---\n",
      "measure     rank       node      score     \n",
      "OutDegree   1          2494      2510      \n",
      "OutDegree   2          4805      2247      \n",
      "OutDegree   3           398      2208      \n",
      "OutDegree   4           381      1850      \n",
      "OutDegree   5           226      1700      \n",
      "OutDegree   6            37      1442      \n",
      "OutDegree   7          5706      1442      \n",
      "OutDegree   8          4826      1216      \n",
      "OutDegree   9          5513       930      \n",
      "OutDegree  10            49       869      \n",
      "OutDegree  11          3482       838      \n",
      "OutDegree  12           217       825      \n",
      "OutDegree  13          2483       742      \n",
      "OutDegree  14          9190       694      \n",
      "OutDegree  15          5685       691      \n",
      "OutDegree  16          5383       685      \n",
      "OutDegree  17         46363       685      \n",
      "OutDegree  18           385       665      \n",
      "OutDegree  19          2830       662      \n",
      "OutDegree  20          5520       639      \n",
      "\n",
      "=== Overlaps among Top-N sets (intersection + Jaccard) ===\n",
      "A              B               intersection_count  jaccard     common_nodes                                         \n",
      "      PageRank       HITS_hub  4                  0.111111                                     [49, 398, 2494, 4805]\n",
      "      PageRank HITS_authority  5                  0.142857                               [49, 398, 2494, 3482, 4805]\n",
      "      PageRank       InDegree 19                  0.904762       [37, 49, 217, 219, 226, 381, 398, 2483, 2494, 2828]\n",
      "      PageRank      OutDegree 16                  0.666667      [37, 49, 217, 226, 381, 398, 2483, 2494, 3482, 4805]\n",
      "      HITS_hub HITS_authority 17                  0.739130     [49, 195, 324, 342, 398, 413, 1509, 1723, 2488, 2491]\n",
      "      HITS_hub       InDegree  4                  0.111111                                     [49, 398, 2494, 4805]\n",
      "      HITS_hub      OutDegree  4                  0.111111                                     [49, 398, 2494, 4805]\n",
      "HITS_authority       InDegree  5                  0.142857                               [49, 398, 2494, 3482, 4805]\n",
      "HITS_authority      OutDegree  5                  0.142857                               [49, 398, 2494, 3482, 4805]\n",
      "      InDegree      OutDegree 16                  0.666667      [37, 49, 217, 226, 381, 398, 2483, 2494, 3482, 4805]\n",
      "\n",
      "=== Commentary (quick read) ===\n",
      "- PageRank tends to favor nodes that receive links from other well-linked nodes\n",
      "  (global prestige / influence).\n",
      "- HITS splits roles:\n",
      "  * Authorities: nodes pointed to by strong hubs (often similar to high in-degree, but weighted).\n",
      "  * Hubs: nodes that point to many good authorities (often similar to high out-degree, but weighted).\n",
      "- In/Out-degree are simple baselines: purely local counts with no recursive weighting.\n",
      "\n",
      "Top-20 overlap counts vs PageRank:\n",
      "  PageRank ∩ HITS_authority: 5\n",
      "  PageRank ∩ HITS_hub:       4\n",
      "  PageRank ∩ InDegree:       19\n",
      "  PageRank ∩ OutDegree:      16\n",
      "\n",
      "Example common nodes (first few):\n",
      "  PageRank & HITS_authority: [49, 398, 2494, 3482, 4805]\n",
      "  PageRank & InDegree:       [37, 49, 217, 219, 226, 381, 398, 2483, 2494, 2828]\n",
      "  PageRank & HITS_hub:       [49, 398, 2494, 4805]\n",
      "  PageRank & OutDegree:      [37, 49, 217, 226, 381, 398, 2483, 2494, 3482, 4805]\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import gzip\n",
    "import io\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Sequence, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import igraph as ig\n",
    "\n",
    "\n",
    "URL = \"https://snap.stanford.edu/data/soc-Slashdot0902.txt.gz\"\n",
    "FILENAME = \"soc-Slashdot0902.txt.gz\"\n",
    "gz_path = \"../data/\" + FILENAME\n",
    "\n",
    "def download_if_needed(url: str, dest: str, chunk_size: int = 1 << 20) -> None:\n",
    "    dest = Path(dest)\n",
    "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if dest.exists() and dest.stat().st_size > 0:\n",
    "        return\n",
    "\n",
    "    print(f\"[download] Fetching: {url}\")\n",
    "    with requests.get(url, stream=True, timeout=120) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(dest, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "    print(f\"[download] Saved to: {dest} ({dest.stat().st_size:,} bytes)\")\n",
    "\n",
    "\n",
    "def iter_edges_from_gz(path: Path) -> Iterable[Tuple[str, str]]:\n",
    "    # File format: comment lines start with '#', edges are \"src dst\"\n",
    "    with gzip.open(path, \"rt\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        for line in f:\n",
    "            if not line or line.startswith(\"#\"):\n",
    "                continue\n",
    "            parts = line.split()\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            yield parts[0], parts[1]\n",
    "\n",
    "\n",
    "def build_graph(edges: Iterable[Tuple[str, str]]) -> ig.Graph:\n",
    "    # TupleList creates vertex \"name\" attributes from IDs in edges.\n",
    "    # Keep directed=True as dataset is directed.\n",
    "    g = ig.Graph.TupleList(edges, directed=True, vertex_name_attr=\"name\")\n",
    "    # Remove self-loops and parallel edges (typical preprocessing)\n",
    "    g.simplify(multiple=True, loops=True, combine_edges=None)\n",
    "    return g\n",
    "\n",
    "\n",
    "def top_n_table(\n",
    "    g: ig.Graph, scores: Sequence[float], measure: str, top_n: int\n",
    ") -> pd.DataFrame:\n",
    "    names = g.vs[\"name\"]\n",
    "    order = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_n]\n",
    "    rows = [{\"measure\": measure, \"node\": int(names[i]), \"score\": float(scores[i]), \"rank\": r + 1}\n",
    "            for r, i in enumerate(order)]\n",
    "    return pd.DataFrame(rows, columns=[\"measure\", \"rank\", \"node\", \"score\"])\n",
    "\n",
    "\n",
    "def compare_top_sets(tops: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "    # Pairwise overlap (intersection size + Jaccard)\n",
    "    measures = list(tops.keys())\n",
    "    top_sets = {m: set(tops[m][\"node\"].tolist()) for m in measures}\n",
    "\n",
    "    rows = []\n",
    "    for i in range(len(measures)):\n",
    "        for j in range(i + 1, len(measures)):\n",
    "            a, b = measures[i], measures[j]\n",
    "            sa, sb = top_sets[a], top_sets[b]\n",
    "            inter = sa & sb\n",
    "            union = sa | sb\n",
    "            rows.append({\n",
    "                \"A\": a,\n",
    "                \"B\": b,\n",
    "                \"intersection_count\": len(inter),\n",
    "                \"jaccard\": (len(inter) / len(union)) if union else 0.0,\n",
    "                \"common_nodes\": sorted(inter)[:10],  # show a few to keep output compact\n",
    "            })\n",
    "    return pd.DataFrame(rows, columns=[\"A\", \"B\", \"intersection_count\", \"jaccard\", \"common_nodes\"])\n",
    "\n",
    "def construct_graph(gz_path):\n",
    "    print(\"[load] Reading edges...\")\n",
    "    edges = list(iter_edges_from_gz(gz_path))\n",
    "    print(f\"[load] Raw edges read: {len(edges):,}\")\n",
    "\n",
    "    print(\"[graph] Building igraph directed graph...\")\n",
    "    g = build_graph(edges)\n",
    "    return g\n",
    "\n",
    "def problem_1():\n",
    "  \n",
    "    \n",
    "    top = 20;\n",
    "    \n",
    "    download_if_needed(URL, gz_path)\n",
    "    \n",
    "    g = construct_graph(gz_path)\n",
    "    n, m = g.vcount(), g.ecount()\n",
    "    print(f\"[graph] Vertices: {n:,}  Edges: {m:,}  (after simplify)\")\n",
    "\n",
    "    # --- Centrality computations (igraph) ---\n",
    "    print(\"[compute] PageRank...\")\n",
    "    pr = g.pagerank(directed=True)\n",
    "\n",
    "    print(\"[compute] HITS hub/authority scores...\")\n",
    "    hubs = g.hub_score()          # directed by default\n",
    "    auth = g.authority_score()    # directed by default\n",
    "\n",
    "    print(\"[compute] In-degree / Out-degree...\")\n",
    "    indeg = g.indegree()\n",
    "    outdeg = g.outdegree()\n",
    "\n",
    "    # --- Top-N tables ---\n",
    "    tops: Dict[str, pd.DataFrame] = {}\n",
    "    tops[\"PageRank\"] = top_n_table(g, pr, \"PageRank\", top)\n",
    "    tops[\"HITS_hub\"] = top_n_table(g, hubs, \"HITS_hub\", top)\n",
    "    tops[\"HITS_authority\"] = top_n_table(g, auth, \"HITS_authority\", top)\n",
    "    tops[\"InDegree\"] = top_n_table(g, indeg, \"InDegree\", top)\n",
    "    tops[\"OutDegree\"] = top_n_table(g, outdeg, \"OutDegree\", top)\n",
    "\n",
    "    print(\"\\n=== Top-20 tables (by measure) ===\")\n",
    "    for measure, df in tops.items():\n",
    "        print(f\"\\n--- {measure} ---\")\n",
    "        # make it readable in terminal\n",
    "        print(df.to_string(index=False, justify=\"left\", col_space=10, formatters={\"score\": \"{:.6g}\".format}))\n",
    "\n",
    "    # --- Overlap / difference commentary ---\n",
    "    overlap_df = compare_top_sets(tops)\n",
    "    print(\"\\n=== Overlaps among Top-N sets (intersection + Jaccard) ===\")\n",
    "    print(overlap_df.to_string(index=False, justify=\"left\", col_space=12))\n",
    "\n",
    "    # A short, human-readable commentary (heuristic / typical differences)\n",
    "    pr_top = set(tops[\"PageRank\"][\"node\"])\n",
    "    hub_top = set(tops[\"HITS_hub\"][\"node\"])\n",
    "    auth_top = set(tops[\"HITS_authority\"][\"node\"])\n",
    "    indeg_top = set(tops[\"InDegree\"][\"node\"])\n",
    "    outdeg_top = set(tops[\"OutDegree\"][\"node\"])\n",
    "\n",
    "    print(\"\\n=== Commentary (quick read) ===\")\n",
    "    print(\n",
    "        \"- PageRank tends to favor nodes that receive links from other well-linked nodes\\n\"\n",
    "        \"  (global prestige / influence).\\n\"\n",
    "        \"- HITS splits roles:\\n\"\n",
    "        \"  * Authorities: nodes pointed to by strong hubs (often similar to high in-degree, but weighted).\\n\"\n",
    "        \"  * Hubs: nodes that point to many good authorities (often similar to high out-degree, but weighted).\\n\"\n",
    "        \"- In/Out-degree are simple baselines: purely local counts with no recursive weighting.\\n\"\n",
    "    )\n",
    "\n",
    "    print(f\"Top-{top} overlap counts vs PageRank:\")\n",
    "    print(f\"  PageRank ∩ HITS_authority: {len(pr_top & auth_top)}\")\n",
    "    print(f\"  PageRank ∩ HITS_hub:       {len(pr_top & hub_top)}\")\n",
    "    print(f\"  PageRank ∩ InDegree:       {len(pr_top & indeg_top)}\")\n",
    "    print(f\"  PageRank ∩ OutDegree:      {len(pr_top & outdeg_top)}\")\n",
    "\n",
    "    # Show a few common nodes to illustrate overlap\n",
    "    common_auth = sorted(pr_top & auth_top)[:10]\n",
    "    common_indeg = sorted(pr_top & indeg_top)[:10]\n",
    "    common_hub = sorted(pr_top & hub_top)[:10]\n",
    "    common_outdeg = sorted(pr_top & outdeg_top)[:10]\n",
    "\n",
    "    print(\"\\nExample common nodes (first few):\")\n",
    "    print(f\"  PageRank & HITS_authority: {common_auth}\")\n",
    "    print(f\"  PageRank & InDegree:       {common_indeg}\")\n",
    "    print(f\"  PageRank & HITS_hub:       {common_hub}\")\n",
    "    print(f\"  PageRank & OutDegree:      {common_outdeg}\")\n",
    "\n",
    "    print(\"\\nDone.\")\n",
    "\n",
    "problem_1()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the tables, we can see there is quite a large overlap between the top PageRank nodes and the top InDegree nodes. In fact, there is only one node that appears in the top 20 page rank table, but doesn't appear in the top 20 in degree table. There is a similar level of overlap with the out degree table.  \n",
    "We also see that there isn't much overlap with the top page rank nodes and the top HITS nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1H9BritIFoH"
   },
   "source": [
    "# 2. Connectivity Structure\n",
    "- Count SCCs and WCCs; inspect the **giant SCC**.\n",
    "\n",
    "- Pick 1–3 **representative SCCs** (by size) and compute internal stats (e.g., degree, density, average path length if feasible).\n",
    "\n",
    "- Discuss what you observe about the structure of this subgraph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries: {'SCC': ComponentSummary(kind='SCC', n_components=3, giant_size=3, giant_fraction=0.5, size_histogram_top=[(3, 1), (2, 1), (1, 1)], sizes_desc=[3, 2, 1]), 'WCC': ComponentSummary(kind='WCC', n_components=3, giant_size=3, giant_fraction=0.5, size_histogram_top=[(3, 1), (2, 1), (1, 1)], sizes_desc=[3, 2, 1])}\n",
      "Giant SCC vcount/ecount: 3 3\n",
      "Representative SCC sizes: [3, 2, 1]\n",
      "Largest SCC stats: SCCInternalStats(scc_rank_by_size=1, size=3, density=0.5, avg_in_degree=1.0, avg_out_degree=1.0, avg_total_degree=2.0, reciprocity=0.0, avg_path_length=1.5, diameter=2)\n",
      "test_case_1_two_cycles_plus_isolate: PASS\n",
      "Summaries: {'SCC': ComponentSummary(kind='SCC', n_components=3, giant_size=4, giant_fraction=0.6666666666666666, size_histogram_top=[(1, 2), (4, 1)], sizes_desc=[4, 1, 1]), 'WCC': ComponentSummary(kind='WCC', n_components=1, giant_size=6, giant_fraction=1.0, size_histogram_top=[(6, 1)], sizes_desc=[6])}\n",
      "Giant SCC vcount/ecount: 4 5\n",
      "Representative SCC sizes: [4, 1, 1]\n",
      "Big SCC stats: SCCInternalStats(scc_rank_by_size=1, size=4, density=0.4166666666666667, avg_in_degree=1.25, avg_out_degree=1.25, avg_total_degree=2.5, reciprocity=0.0, avg_path_length=1.75, diameter=3)\n",
      "test_case_2_one_big_scc_with_tail: PASS\n",
      "[load] Reading edges...\n",
      "[load] Raw edges read: 948,464\n",
      "[graph] Building igraph directed graph...\n",
      "The directed graph contains 10,559 SCCs. The giant SCC has 71,307 nodes (86.78% of all nodes).\n",
      "SCC rank #1: size=71,307, density=0.00016544, avg_in=11.8, avg_out=11.8, avg_total=23.6, reciprocity=0.869, path stats skipped/too expensive\n",
      "SCC rank #2: size=6, density=0.333333, avg_in=1.67, avg_out=1.67, avg_total=3.33, reciprocity=1, avg_path_len=2.07, diam=4\n",
      "SCC rank #3: size=6, density=0.7, avg_in=3.5, avg_out=3.5, avg_total=7, reciprocity=0.952, avg_path_len=1.3, diam=2\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import igraph as ig\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ComponentSummary:\n",
    "    kind: str  # \"SCC\" or \"WCC\"\n",
    "    n_components: int\n",
    "    giant_size: int\n",
    "    giant_fraction: float\n",
    "    size_histogram_top: List[Tuple[int, int]]  # (component_size, count), top few\n",
    "    sizes_desc: List[int]  # full sizes sorted desc (can be large; keep if you want)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SCCInternalStats:\n",
    "    scc_rank_by_size: int  # 1 = largest\n",
    "    size: int\n",
    "    density: float\n",
    "    avg_in_degree: float\n",
    "    avg_out_degree: float\n",
    "    avg_total_degree: float\n",
    "    reciprocity: Optional[float]  # None if not meaningful / fails\n",
    "    avg_path_length: Optional[float]  # None if not feasible / fails\n",
    "    diameter: Optional[int]  # None if not feasible / fails\n",
    "\n",
    "\n",
    "def _top_histogram(sizes: List[int], top_k: int = 10) -> List[Tuple[int, int]]:\n",
    "    # returns top_k most frequent sizes, tie-broken by size desc\n",
    "    from collections import Counter\n",
    "\n",
    "    c = Counter(sizes)\n",
    "    items = sorted(c.items(), key=lambda x: (x[1], x[0]), reverse=True)\n",
    "    return items[:top_k]\n",
    "\n",
    "\n",
    "def count_components_and_giant_scc(g: ig.Graph) -> Dict[str, ComponentSummary]:\n",
    "    \"\"\"\n",
    "    Count SCCs and WCCs and summarize component size distribution.\n",
    "    Also identifies the giant SCC size/fraction (largest strongly connected component).\n",
    "\n",
    "    Returns dict with keys: \"SCC\", \"WCC\".\n",
    "    \"\"\"\n",
    "    if not g.is_directed():\n",
    "        raise ValueError(\"Expected a directed graph for SCC analysis.\")\n",
    "\n",
    "    # Strongly connected components\n",
    "    scc = g.components(mode=\"STRONG\")\n",
    "    scc_sizes = scc.sizes()\n",
    "    scc_sizes_desc = sorted(scc_sizes, reverse=True)\n",
    "    giant_scc_size = scc_sizes_desc[0] if scc_sizes_desc else 0\n",
    "\n",
    "    # Weakly connected components\n",
    "    wcc = g.components(mode=\"WEAK\")\n",
    "    wcc_sizes = wcc.sizes()\n",
    "    wcc_sizes_desc = sorted(wcc_sizes, reverse=True)\n",
    "    giant_wcc_size = wcc_sizes_desc[0] if wcc_sizes_desc else 0\n",
    "\n",
    "    n = g.vcount() or 1  # avoid div-by-zero\n",
    "\n",
    "    return {\n",
    "        \"SCC\": ComponentSummary(\n",
    "            kind=\"SCC\",\n",
    "            n_components=len(scc_sizes),\n",
    "            giant_size=giant_scc_size,\n",
    "            giant_fraction=giant_scc_size / n,\n",
    "            size_histogram_top=_top_histogram(scc_sizes, top_k=10),\n",
    "            sizes_desc=scc_sizes_desc,\n",
    "        ),\n",
    "        \"WCC\": ComponentSummary(\n",
    "            kind=\"WCC\",\n",
    "            n_components=len(wcc_sizes),\n",
    "            giant_size=giant_wcc_size,\n",
    "            giant_fraction=giant_wcc_size / n,\n",
    "            size_histogram_top=_top_histogram(wcc_sizes, top_k=10),\n",
    "            sizes_desc=wcc_sizes_desc,\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_giant_scc(g: ig.Graph) -> ig.Graph:\n",
    "    \"\"\"\n",
    "    Return the induced subgraph corresponding to the giant (largest) SCC.\n",
    "    Keeps vertex attributes (including 'name' if present).\n",
    "    \"\"\"\n",
    "    if not g.is_directed():\n",
    "        raise ValueError(\"Expected a directed graph.\")\n",
    "    scc = g.components(mode=\"STRONG\")\n",
    "    if scc.giant() is None or g.vcount() == 0:\n",
    "        return g.subgraph([])  # empty graph\n",
    "    return scc.giant()  # igraph returns the subgraph directly\n",
    "\n",
    "\n",
    "def _safe_reciprocity(h: ig.Graph) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Reciprocity is meaningful for directed graphs; igraph has a reciprocity() method\n",
    "    in recent versions. If unavailable or fails, return None.\n",
    "    \"\"\"\n",
    "    if not h.is_directed() or h.ecount() == 0:\n",
    "        return None\n",
    "    try:\n",
    "        # igraph API differs across versions; try method then fallback\n",
    "        if hasattr(h, \"reciprocity\"):\n",
    "            return float(h.reciprocity())\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "\n",
    "def _path_stats_if_feasible(\n",
    "    h: ig.Graph,\n",
    "    max_vertices: int = 5000,\n",
    "    max_edges: int = 200000,\n",
    "    directed: bool = True,\n",
    ") -> Tuple[Optional[float], Optional[int]]:\n",
    "    \"\"\"\n",
    "    Average path length and diameter can be expensive.\n",
    "    Compute only if size is reasonable; otherwise return (None, None).\n",
    "    For SCCs, directed paths are well-defined; you can set directed=False if desired.\n",
    "\n",
    "    Note: For very large SCCs this can still be expensive.\n",
    "    \"\"\"\n",
    "    if h.vcount() == 0:\n",
    "        return None, None\n",
    "    if h.vcount() > max_vertices or h.ecount() > max_edges:\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        apl = float(h.average_path_length(directed=directed, unconn=False))\n",
    "    except Exception:\n",
    "        apl = None\n",
    "\n",
    "    try:\n",
    "        diam = int(h.diameter(directed=directed, unconn=False))\n",
    "    except Exception:\n",
    "        diam = None\n",
    "\n",
    "    return apl, diam\n",
    "\n",
    "\n",
    "def pick_representative_sccs(\n",
    "    g: ig.Graph,\n",
    "    ranks_by_size: List[int] = [1, 2, 5],\n",
    "    min_size: int = 10,\n",
    ") -> List[ig.Graph]:\n",
    "    \"\"\"\n",
    "    Pick 1–3 representative SCCs by size rank (1 = largest).\n",
    "    Filters out SCCs smaller than min_size. Returns subgraphs.\n",
    "\n",
    "    Example:\n",
    "      reps = pick_representative_sccs(g, ranks_by_size=[1, 3, 10], min_size=50)\n",
    "    \"\"\"\n",
    "    if not g.is_directed():\n",
    "        raise ValueError(\"Expected a directed graph.\")\n",
    "\n",
    "    scc = g.components(mode=\"STRONG\")\n",
    "    sizes = scc.sizes()\n",
    "    if not sizes:\n",
    "        return []\n",
    "\n",
    "    # Get SCC indices sorted by size desc\n",
    "    order = sorted(range(len(sizes)), key=lambda i: sizes[i], reverse=True)\n",
    "\n",
    "    selected: List[ig.Graph] = []\n",
    "    for r in ranks_by_size:\n",
    "        idx_in_order = r - 1\n",
    "        if idx_in_order < 0 or idx_in_order >= len(order):\n",
    "            continue\n",
    "        comp_idx = order[idx_in_order]\n",
    "        if sizes[comp_idx] < min_size:\n",
    "            continue\n",
    "        verts = scc[comp_idx]  # vertex indices in this SCC\n",
    "        selected.append(g.subgraph(verts))\n",
    "\n",
    "        if len(selected) >= 3:\n",
    "            break\n",
    "\n",
    "    return selected\n",
    "\n",
    "\n",
    "def compute_scc_internal_stats(\n",
    "    scc_subgraph: ig.Graph,\n",
    "    scc_rank_by_size: int,\n",
    "    path_stats: bool = True,\n",
    "    path_max_vertices: int = 5000,\n",
    "    path_max_edges: int = 200000,\n",
    ") -> SCCInternalStats:\n",
    "    \"\"\"\n",
    "    Compute internal stats for a given SCC subgraph:\n",
    "      - size\n",
    "      - density\n",
    "      - average in/out/total degree\n",
    "      - reciprocity (if available)\n",
    "      - average path length + diameter (if feasible)\n",
    "\n",
    "    Assumes scc_subgraph is strongly connected (but doesn't enforce).\n",
    "    \"\"\"\n",
    "    n = scc_subgraph.vcount()\n",
    "    m = scc_subgraph.ecount()\n",
    "\n",
    "    indeg = scc_subgraph.indegree() if n else []\n",
    "    outdeg = scc_subgraph.outdegree() if n else []\n",
    "    totaldeg = scc_subgraph.degree(mode=\"ALL\") if n else []\n",
    "\n",
    "    density = float(scc_subgraph.density(loops=False)) if n else 0.0\n",
    "\n",
    "    reciprocity = _safe_reciprocity(scc_subgraph)\n",
    "\n",
    "    apl, diam = (None, None)\n",
    "    if path_stats:\n",
    "        apl, diam = _path_stats_if_feasible(\n",
    "            scc_subgraph,\n",
    "            max_vertices=path_max_vertices,\n",
    "            max_edges=path_max_edges,\n",
    "            directed=True,\n",
    "        )\n",
    "\n",
    "    return SCCInternalStats(\n",
    "        scc_rank_by_size=scc_rank_by_size,\n",
    "        size=n,\n",
    "        density=density,\n",
    "        avg_in_degree=(sum(indeg) / n) if n else 0.0,\n",
    "        avg_out_degree=(sum(outdeg) / n) if n else 0.0,\n",
    "        avg_total_degree=(sum(totaldeg) / n) if n else 0.0,\n",
    "        reciprocity=reciprocity,\n",
    "        avg_path_length=apl,\n",
    "        diameter=diam,\n",
    "    )\n",
    "\n",
    "\n",
    "def summarize_scc_structure(\n",
    "    scc_stats: List[SCCInternalStats],\n",
    "    global_scc_summary: Optional[ComponentSummary] = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Produce a short narrative based on SCC internal stats and optional global SCC summary.\n",
    "    You can print/attach this to a report.\n",
    "\n",
    "    This is heuristic commentary—not a proof.\n",
    "    \"\"\"\n",
    "    lines: List[str] = []\n",
    "\n",
    "    if global_scc_summary is not None:\n",
    "        lines.append(\n",
    "            f\"The directed graph contains {global_scc_summary.n_components:,} SCCs. \"\n",
    "            f\"The giant SCC has {global_scc_summary.giant_size:,} nodes \"\n",
    "            f\"({global_scc_summary.giant_fraction:.2%} of all nodes).\"\n",
    "        )\n",
    "\n",
    "    if not scc_stats:\n",
    "        lines.append(\"No SCCs met the selection criteria (or the graph is empty).\")\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "    # Sort by rank (1 = largest)\n",
    "    scc_stats_sorted = sorted(scc_stats, key=lambda x: x.scc_rank_by_size)\n",
    "\n",
    "    for st in scc_stats_sorted:\n",
    "        tag = f\"SCC rank #{st.scc_rank_by_size}\"\n",
    "        lines.append(\n",
    "            f\"{tag}: size={st.size:,}, density={st.density:.6g}, \"\n",
    "            f\"avg_in={st.avg_in_degree:.3g}, avg_out={st.avg_out_degree:.3g}, \"\n",
    "            f\"avg_total={st.avg_total_degree:.3g}\"\n",
    "            + (f\", reciprocity={st.reciprocity:.3g}\" if st.reciprocity is not None else \"\")\n",
    "            + (f\", avg_path_len={st.avg_path_length:.3g}, diam={st.diameter}\"\n",
    "               if st.avg_path_length is not None and st.diameter is not None else\n",
    "               \", path stats skipped/too expensive\")\n",
    "        )\n",
    "\n",
    "    \n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "def test_case_1_two_cycles_plus_isolate():\n",
    "    \"\"\"\n",
    "    Graph:\n",
    "      - SCC A: 0->1->2->0 (size 3)\n",
    "      - SCC B: 3<->4 (size 2)\n",
    "      - SCC C: 5 alone (size 1)\n",
    "    Weakly connected components:\n",
    "      - {0,1,2} size 3\n",
    "      - {3,4} size 2\n",
    "      - {5} size 1\n",
    "    \"\"\"\n",
    "    g = ig.Graph(directed=True)\n",
    "    g.add_vertices(6)\n",
    "    g.add_edges([\n",
    "        (0, 1), (1, 2), (2, 0),   # 3-cycle\n",
    "        (3, 4), (4, 3),           # 2-cycle\n",
    "        # vertex 5 isolated\n",
    "    ])\n",
    "\n",
    "    summaries = count_components_and_giant_scc(g)\n",
    "    print(\"Summaries:\", summaries)\n",
    "\n",
    "    # SCC summary checks\n",
    "    scc_sum = summaries[\"SCC\"]\n",
    "    assert scc_sum.n_components == 3, f\"Expected 3 SCCs, got {scc_sum.n_components}\"\n",
    "    assert scc_sum.giant_size == 3, f\"Expected giant SCC size 3, got {scc_sum.giant_size}\"\n",
    "    assert abs(scc_sum.giant_fraction - (3/6)) < 1e-12\n",
    "\n",
    "    # WCC summary checks\n",
    "    wcc_sum = summaries[\"WCC\"]\n",
    "    assert wcc_sum.n_components == 3, f\"Expected 3 WCCs, got {wcc_sum.n_components}\"\n",
    "    assert wcc_sum.giant_size == 3, f\"Expected giant WCC size 3, got {wcc_sum.giant_size}\"\n",
    "\n",
    "    # Giant SCC extraction\n",
    "    giant = extract_giant_scc(g)\n",
    "    print(\"Giant SCC vcount/ecount:\", giant.vcount(), giant.ecount())\n",
    "    assert giant.vcount() == 3, f\"Expected giant SCC to have 3 vertices, got {giant.vcount()}\"\n",
    "\n",
    "    # Representative SCC picking: ranks 1..3 with min_size=0 should return all 3 SCCs\n",
    "    reps = pick_representative_sccs(g, ranks_by_size=[1, 2, 3], min_size=0)\n",
    "    rep_sizes = [h.vcount() for h in reps]\n",
    "    print(\"Representative SCC sizes:\", rep_sizes)\n",
    "    assert rep_sizes == [3, 2, 1], f\"Expected [3,2,1], got {rep_sizes}\"\n",
    "\n",
    "    # Internal stats sanity for the largest SCC (3-cycle)\n",
    "    st = compute_scc_internal_stats(reps[0], scc_rank_by_size=1, path_stats=True)\n",
    "    print(\"Largest SCC stats:\", st)\n",
    "\n",
    "    assert st.size == 3\n",
    "    assert abs(st.avg_in_degree - 1.0) < 1e-12\n",
    "    assert abs(st.avg_out_degree - 1.0) < 1e-12\n",
    "    assert abs(st.avg_total_degree - 2.0) < 1e-12\n",
    "    # In a directed 3-cycle, average path length is 1.5 and diameter is 2 (directed shortest paths)\n",
    "    if st.avg_path_length is not None:\n",
    "        assert abs(st.avg_path_length - 1.5) < 1e-12, f\"Expected APL 1.5, got {st.avg_path_length}\"\n",
    "    if st.diameter is not None:\n",
    "        assert st.diameter == 2, f\"Expected diam 2, got {st.diameter}\"\n",
    "\n",
    "    print(\"test_case_1_two_cycles_plus_isolate: PASS\")\n",
    "\n",
    "\n",
    "def test_case_2_one_big_scc_with_tail():\n",
    "    \"\"\"\n",
    "    Graph:\n",
    "      - Big SCC: 0,1,2,3 form a strongly-connected 4-node component\n",
    "      - Tail: 3 -> 4 -> 5 (no edges back), so {4}, {5} are singleton SCCs\n",
    "    Weakly connected components: all nodes connected (size 6) because direction ignored.\n",
    "    \"\"\"\n",
    "    g = ig.Graph(directed=True)\n",
    "    g.add_vertices(6)\n",
    "\n",
    "    # Make 0-1-2-3 strongly connected (cycle) plus some extra edges\n",
    "    g.add_edges([\n",
    "        (0, 1), (1, 2), (2, 3), (3, 0),  # 4-cycle => SCC size 4\n",
    "        (0, 2),                           # chord\n",
    "        (3, 4), (4, 5)                    # tail out of SCC\n",
    "    ])\n",
    "\n",
    "    summaries = count_components_and_giant_scc(g)\n",
    "    print(\"Summaries:\", summaries)\n",
    "\n",
    "    scc_sum = summaries[\"SCC\"]\n",
    "    wcc_sum = summaries[\"WCC\"]\n",
    "\n",
    "    # SCC: {0,1,2,3} plus {4} plus {5} => 3 SCCs\n",
    "    assert scc_sum.n_components == 3, f\"Expected 3 SCCs, got {scc_sum.n_components}\"\n",
    "    assert scc_sum.giant_size == 4, f\"Expected giant SCC size 4, got {scc_sum.giant_size}\"\n",
    "    assert abs(scc_sum.giant_fraction - (4/6)) < 1e-12\n",
    "\n",
    "    # WCC: direction ignored => everything is connected via 3-4-5, so 1 WCC of size 6\n",
    "    assert wcc_sum.n_components == 1, f\"Expected 1 WCC, got {wcc_sum.n_components}\"\n",
    "    assert wcc_sum.giant_size == 6, f\"Expected giant WCC size 6, got {wcc_sum.giant_size}\"\n",
    "    assert abs(wcc_sum.giant_fraction - 1.0) < 1e-12\n",
    "\n",
    "    giant = extract_giant_scc(g)\n",
    "    print(\"Giant SCC vcount/ecount:\", giant.vcount(), giant.ecount())\n",
    "    assert giant.vcount() == 4, f\"Expected giant SCC to have 4 vertices, got {giant.vcount()}\"\n",
    "\n",
    "    # Pick reps: rank 1 is size 4; ranks 2/3 are singletons\n",
    "    reps = pick_representative_sccs(g, ranks_by_size=[1, 2, 3], min_size=0)\n",
    "    rep_sizes = [h.vcount() for h in reps]\n",
    "    print(\"Representative SCC sizes:\", rep_sizes)\n",
    "    assert rep_sizes[0] == 4\n",
    "    assert sorted(rep_sizes[1:]) == [1, 1]\n",
    "\n",
    "    # Basic internal stats on the big SCC\n",
    "    st = compute_scc_internal_stats(reps[0], scc_rank_by_size=1, path_stats=True)\n",
    "    print(\"Big SCC stats:\", st)\n",
    "    assert st.size == 4\n",
    "    assert st.avg_in_degree > 0\n",
    "    assert st.avg_out_degree > 0\n",
    "    assert st.avg_total_degree > 0\n",
    "\n",
    "    print(\"test_case_2_one_big_scc_with_tail: PASS\")\n",
    "\n",
    "test_case_1_two_cycles_plus_isolate()\n",
    "test_case_2_one_big_scc_with_tail()\n",
    "\n",
    "def problem2():\n",
    "    g = construct_graph(gz_path)\n",
    "    summaries = count_components_and_giant_scc(g)\n",
    "    giant = extract_giant_scc(g)\n",
    "    \n",
    "    rep_sccs = pick_representative_sccs(g, ranks_by_size=[1, 2, 3], min_size=0)\n",
    "    \n",
    "    stats = []\n",
    "    for i, sub in enumerate(rep_sccs, start=1):\n",
    "        stats.append(compute_scc_internal_stats(sub, scc_rank_by_size=i, path_stats=True))\n",
    "    \n",
    "    commentary = summarize_scc_structure(stats, global_scc_summary=summaries[\"SCC\"])\n",
    "    print(commentary)\n",
    "\n",
    "problem2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observed the following:  \n",
    "- The largest SCC contains approximately 90% of all nodes\n",
    "- The largest SCC is not very dense, but has high reciprocity\n",
    "- The largest SCC is several orders of magnitude greater than the next largest SCC in terms of size\n",
    "- The other SCCs have much higher density, but this is probably just because of the differences in size\n",
    "\n",
    "Thus we have a very sparsely connected SCC with tens of thousands of nodes where most nodes have mutual edges to one another. All other SCCs in the graph contain less than 10 nodes, and thus don't carry much meaning since high density is quite easy to achieve with fewer nodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1pFhyD3dIFzU"
   },
   "source": [
    "# 3. Local Community via Personalized PageRank (PPR)\n",
    "Pick a **seed node** (e.g., top PageRank, or a node of interest). Compute **personalized PageRank** with a restart vector focused on that node (or a small seed set).\n",
    "\n",
    "- Extract a **local community** by thresholding PPR scores (e.g., top-`k` or score ≥ τ).\n",
    "- Visualize the induced subgraph and **describe** the community's characteristics.\n",
    "- Compare with a baseline (e.g., ego‑net, or Louvain on a symmetrized graph)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[load] Reading edges...\n",
      "[load] Raw edges read: 948,464\n",
      "[graph] Building igraph directed graph...\n",
      "21086\n",
      "20818\n"
     ]
    }
   ],
   "source": [
    "def find_seed_node(g):\n",
    "    page_ranks = g.pagerank()\n",
    "    table = top_n_table(g, page_ranks, \"PageRank\", 1)\n",
    "    return table.iloc[0][\"node\"];\n",
    "\n",
    "def display_graph(g):\n",
    "    # --- Plot ---\n",
    "    layout = induced_subgraph.layout(\"fr\")  # Fruchterman-Reingold\n",
    "\n",
    "    # If you have vertex names, show them; otherwise don't label\n",
    "    if \"name\" in induced_subgraph.vs.attributes():\n",
    "        induced_subgraph.vs[\"label\"] = induced_subgraph.vs[\"name\"]\n",
    "    else:\n",
    "        induced_subgraph.vs[\"label\"] = [str(v.index) for v in induced_subgraph.vs]\n",
    "\n",
    "    # Make it readable (small-ish labels if k is large)\n",
    "    visual_style = {\n",
    "        \"layout\": layout,\n",
    "        \"vertex_size\": 18,\n",
    "        \"vertex_label_size\": 10 if k <= 50 else 6,\n",
    "        \"edge_arrow_size\": 0.4,\n",
    "        \"bbox\": (900, 700),\n",
    "        \"margin\": 40,\n",
    "    }\n",
    "\n",
    "    plot = ig.plot(induced_subgraph, **visual_style)\n",
    "    display(plot)\n",
    "\n",
    "def subgraph_volume(subgraph_vertices, g):\n",
    "    out_degrees = g.outdegree()\n",
    "    volume = 0\n",
    "    for vertex in subgraph_vertices:\n",
    "        volume += out_degrees[vertex]\n",
    "    return volume\n",
    "\n",
    "def subgraph_cut(subgraph_vertices, g):\n",
    "    cut = 0;\n",
    "    for vertex in subgraph_vertices:\n",
    "        neighbors = g.neighborhood(vertex, mode=\"out\")\n",
    "        for neighbor in neighbors:\n",
    "            if(not(neighbor in subgraph_vertices)):\n",
    "                cut += 1\n",
    "    return cut\n",
    "\n",
    "def problem3(k: int):\n",
    "    g = construct_graph(gz_path)\n",
    "    seed_node = find_seed_node(g)\n",
    "    personalized_page_ranks = g.personalized_pagerank(reset_vertices=[seed_node])\n",
    "    pagerank_vertex_pairs = [[personalized_page_ranks[i], i] for i in range(0, len(personalized_page_ranks))]\n",
    "    sorted_page_rank = sorted(pagerank_vertex_pairs, key = lambda item: item[0], reverse= True)\n",
    "    top_k_nodes = [value[1] for value in sorted_page_rank[0:k]]\n",
    "\n",
    "    induced_subgraph = g.induced_subgraph(top_k_nodes)\n",
    "    #display_graph(g)\n",
    "    volume = subgraph_volume(top_k_nodes, g)\n",
    "    print(volume)\n",
    "    cut = subgraph_cut(top_k_nodes, g)\n",
    "    print(cut)\n",
    "    \n",
    "    \n",
    "problem3(k = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hyyTNPsSIOKM"
   },
   "source": [
    "# 4. Choose‑Your‑Own\n",
    "\n",
    "For this last problem, you should complete **any one of the listed tasks**.\n",
    "\n",
    "The tasks are:\n",
    "\n",
    "A) **Link Prediction (directed)**: Create train/test splits by removing a subset of edges, then predict likely edges using features such as *Common Neighbors* (on symmetrized graph), *Jaccard*, *Adamic‑Adar*, *Preferential Attachment*, or **Katz** scores. Evaluate with **AUC / Precision@k**.\n",
    "\n",
    "B) **HITS vs PageRank**: Compare top authorities/hubs with PageRank. When do they diverge? Provide examples and reasoning.\n",
    "\n",
    "C) **Temporal or Reciprocity Analysis** (if timestamps exist): Examine edge reciprocity patterns over degree buckets. Hypothesize causality or social/semantic mechanisms.\n",
    "\n",
    "D) **LSH/MinHash for neighborhood similarity**: For large graphs, sketch node neighborhoods and retrieve near‑duplicates (high Jaccard). Assess quality and speed vs exact Jaccard.\n",
    "\n",
    "E) **Bow‑tie Decomposition** (web‑like graphs): Partition nodes into IN/SCC/OUT/tendrils and visualize proportions.\n",
    "\n",
    "F) **Scaling**: For big graphs, compare `networkx` vs `igraph` runtime/memory on PageRank.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5cqkoNzB27p"
   },
   "source": [
    "# B [24pts]. Interview Questions\n",
    "\n",
    "We now pretend this is a real job interview. Here's some guidance on how to answer these questions:\n",
    "\n",
    "1. Briefly restate the question and state any assumptions you are making.\n",
    "\n",
    "2. Explain your reasoning out loud, focusing on tradeoffs, limitations, and constraints.\n",
    "\n",
    "3. As a principle, keep your answers as short and clear as they can be (while still answering the question).\n",
    "\n",
    "4. Write/speak in a conversational but professional tone (avoid being overly formal). For speaking: speak at a reasonable pace and volume, speak clearly, pause when you need to, and practice making \"eye contact\" with the camera. Keep a confident, positive, and professional tone. *For additional coaching and practice, the University Writing Center provides individual appointments: https://writingcenter.tamu.edu/make-an-appointment.*\n",
    "\n",
    "There may not be a single correct answer. We are grading whether your reasoning is reasonable and aware of limitations.\n",
    "\n",
    "\n",
    "**Rubric**\n",
    "\n",
    "[8pt] Clear understanding of the question; reasonable assumptions; thoughtful reasoning that acknowledges tradeoffs and limitations; clear, concise communication in a conversational but professional tone (for speaking: clear pace, volume, and articulation).\n",
    "\n",
    "[4pt] Basic understanding but shallow reasoning or unclear assumptions; communication is somewhat unclear, overly verbose, or overly informal/formal.\n",
    "\n",
    "[0pt] Minimal, unclear, or incorrect response; poor communication or unprofessional tone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kn4FqXDlByUC"
   },
   "source": [
    "# 1.\n",
    "Explain \"random walks\" to me -- what do they do, why do we use them algorithmically, when would we use them?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFQKHmc3KZiU"
   },
   "source": [
    "# 2.\n",
    "The notion of a “community” is defined structurally as dense internal connectivity and sparse external connectivity. What real-world phenomena does this definition capture well, and what important types of communities does it fail to capture?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZlnNs746c-J"
   },
   "source": [
    "# 3.\n",
    "As a video (reminder to keep it brief): So, I see you did a graph analysis (referring to this homework). That's cool -- can you walk me through what you did?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BOimHuE4H7D"
   },
   "source": [
    "# C [4pts]. What new questions do you have?\n",
    "We want you to think bigger! Tell us what questions and curiosity this homework brings up for you.\n",
    "\n",
    "**Rubric**\n",
    "\n",
    "[4pt] Complete, thoughtful response.\n",
    "\n",
    "[2pt] Partial response.\n",
    "\n",
    "[0pt] Minimal response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1FQ3Cys4J2P"
   },
   "source": [
    "# 1.\n",
    "What new questions do you have? Or, what topics are you curious about now? List at least 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iHfAXzasB-LK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
